# test_cluster_complet_corrige.py
from pyspark.sql import SparkSession
import time

print("="*50)
print("ğŸš€ TEST DU CLUSTER SPARK MULTI-NODE")
print("="*50)

# Connexion au cluster
spark = SparkSession.builder \
    .appName("Test_Cluster_PFE") \
    .master("spark://localhost:7077") \
    .config("spark.executor.memory", "2g") \
    .config("spark.executor.cores", "2") \
    .config("spark.sql.shuffle.partitions", "4") \
    .getOrCreate()

print("\nâœ… CONNEXION Ã‰TABLIE")
print("="*50)

# Infos de base
print(f"\nğŸ“Š Version Spark: {spark.version}")
print(f"ğŸ“Š Master URL: {spark.sparkContext.master}")
print(f"ğŸ“Š Application ID: {spark.sparkContext.applicationId}")

# Test 1: Compter les exÃ©cuteurs (mÃ©thode simple)
print("\nğŸ” RECHERCHE DES EXÃ‰CUTEURS...")
try:
    # MÃ©thode 1: via l'interface web
    print("   â€¢ VÃ©rifie sur http://localhost:8080")
    print("   â€¢ Tu dois voir ton worker connectÃ© !")
    
    # MÃ©thode 2: via SparkContext
    executors = spark.sparkContext._jsc.sc().getExecutorMemoryStatus().keys()
    executor_list = list(executors)
    print(f"   â€¢ Nombre d'exÃ©cuteurs trouvÃ©s: {len(executor_list)}")
    
    for i, executor in enumerate(executor_list):
        print(f"      - ExÃ©cuteur {i}: {executor}")
        
except Exception as e:
    print(f"   âš ï¸ MÃ©thode directe: {e}")
    print("   âœ… Utilise l'interface web pour vÃ©rifier")

# Test 2: Calcul distribuÃ©
print("\nâš¡ TEST DE CALCUL DISTRIBUÃ‰")
print("-"*30)

# CrÃ©er un gros DataFrame
print("   CrÃ©ation d'un DataFrame de 10M lignes...")
debut = time.time()
df = spark.range(0, 10000000)
fin_creation = time.time()
print(f"   âœ… CrÃ©Ã© en {fin_creation-debut:.2f} secondes")

# Compter
print("   Comptage en cours...")
debut_count = time.time()
count = df.count()
fin_count = time.time()
print(f"   âœ… {count:,} lignes comptÃ©es en {fin_count-debut_count:.2f} secondes")

# Test 3: OpÃ©ration de groupBy
print("\nğŸ“Š TEST DE GROUPBY DISTRIBUÃ‰")
print("-"*30)

# CrÃ©er des donnÃ©es avec clÃ©s
print("   CrÃ©ation de donnÃ©es avec clÃ©s...")
df2 = spark.range(0, 1000000).selectExpr("id", "id % 5 as key")
print("   Calcul du groupBy...")
debut_group = time.time()
resultat = df2.groupBy("key").count().collect()
fin_group = time.time()
print(f"   âœ… GroupBy terminÃ© en {fin_group-debut_group:.2f} secondes")

for row in resultat:
    print(f"      ClÃ© {row['key']}: {row['count']} lignes")

print("\n" + "="*50)
print("ğŸ‰ TEST TERMINÃ‰ AVEC SUCCÃˆS !")
print("="*50)
print("\nğŸ“Œ VÃ‰RIFICATION FINALE:")
print("   1. Ouvre http://localhost:8080 dans ton navigateur")
print("   2. Tu dois voir le worker connectÃ©")
print("   3. VÃ©rifie que l'application 'Test_Cluster_PFE' apparaÃ®t")

spark.stop()
print("\nâœ… Spark arrÃªtÃ©")