# # test_chargement.py - VERSION FINALE CORRIGÃ‰E
# from pyspark.sql import SparkSession
# import pandas as pd

# print("ğŸš€ DÃ©marrage...")

# # Initialiser Spark
# spark = SparkSession.builder \
#     .appName("Test_Telecom") \
#     .master("local[*]") \
#     .getOrCreate()

# print("âœ… Spark dÃ©marrÃ©")

# # Charger votre Excel avec header=1 (2Ã¨me ligne = noms des colonnes)
# print("ğŸ“‚ Chargement du fichier Excel...")
# chemin_fichier = "../donnees/brutes/Social-Media-Analytics.xlsx"

# # ğŸ”´ CORRECTION : Ajouter header=1
# pandas_df = pd.read_excel(chemin_fichier, header=1)
# print(f"âœ… {len(pandas_df)} lignes chargÃ©es avec pandas")

# # Afficher les noms des colonnes (maintenant corrects)
# print(f"\nğŸ“‹ Noms des colonnes : {list(pandas_df.columns)}")

# # Afficher la PREMIÃˆRE ligne (ligne 0)
# print("\nğŸ“Œ PREMIÃˆRE ligne (index 0) :")
# print(pandas_df.iloc[0])

# # Afficher la DERNIÃˆRE ligne
# print("\nğŸ“Œ DERNIÃˆRE ligne (index -1) :")
# print(pandas_df.iloc[-1])

# # Afficher le nombre total de lignes
# print(f"\nğŸ“Š Total : {len(pandas_df)} lignes (de 0 Ã  {len(pandas_df)-1})")

# # Convertir en Spark
# df = spark.createDataFrame(pandas_df)
# print(f"âœ… {df.count()} lignes dans Spark")

# # Afficher les premiÃ¨res lignes
# print("\nğŸ“Š AperÃ§u des donnÃ©es (premiÃ¨res lignes) :")
# df.show(5, truncate=50)

# # Afficher les derniÃ¨res lignes en Spark
# print("\nğŸ“Š AperÃ§u des donnÃ©es (derniÃ¨res lignes) :")
# df.tail(5)  # Affiche les 5 derniÃ¨res lignes

# # Maintenant les colonnes sont correctes, on peut faire des analyses
# print("\nğŸ“ˆ Distribution par rÃ©seau social :")
# df.groupBy(pandas_df.columns[1]).count().show()  # Utilise le nom rÃ©el de la colonne

# print("\nğŸ‰ Tout est prÃªt !")
# spark.stop()
import os
import sys
from pyspark.sql import SparkSession
import pandas as pd

# --- CONFIGURATION MULTI-NODE FIXE ---
# 1. Le chemin pour TON PC (Driver)
os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable 

# 2. Le chemin pour le CONTENEUR DOCKER (Worker)
# Dans le conteneur, Python est simplement dans /usr/bin/python3
os.environ['PYSPARK_PYTHON'] = 'python3' 

# 3. On ignore toujours le petit dÃ©calage 3.10 / 3.12
os.environ['PYSPARK_IGNORE_VERSION_MISMATCH'] = '1'

print("ğŸš€ DÃ©marrage du mode Multi-node (Correction des chemins)...")

# Initialiser Spark
spark = SparkSession.builder \
    .appName("Test_Telecom_MultiNode") \
    .master("spark://localhost:7077") \
    .config("spark.executor.memory", "1g") \
    .getOrCreate()
print("âœ… Spark connectÃ© au cluster Master")

# Charger votre Excel
print("ğŸ“‚ Chargement du fichier Excel...")
chemin_fichier = "../donnees/brutes/Social-Media-Analytics1.xlsx"

try:
    pandas_df = pd.read_excel(chemin_fichier, header=1)
    print(f"âœ… {len(pandas_df)} lignes chargÃ©es avec pandas")

    # Convertir en Spark (C'est ici que le travail est envoyÃ© au Worker Docker)
    print("âš™ï¸ Distribution des donnÃ©es vers les Workers...")
    df = spark.createDataFrame(pandas_df)
    
    print(f"âœ… {df.count()} lignes distribuÃ©es dans Spark")

    # AperÃ§u
    print("\nğŸ“Š AperÃ§u des donnÃ©es via le Cluster :")
    df.show(5, truncate=50)

    # Distribution par rÃ©seau social (Calcul fait par le Worker)
    print("\nğŸ“ˆ Distribution par rÃ©seau social (Calcul distribuÃ©) :")
    colonne_reseau = pandas_df.columns[1]
    df.groupBy(colonne_reseau).count().show()

except Exception as e:
    print(f"âŒ Erreur lors du traitement : {e}")

print("\nğŸ‰ Test Multi-node terminÃ© !")
spark.stop()