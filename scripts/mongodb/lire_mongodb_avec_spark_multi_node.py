#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script pour lire MongoDB avec Spark multi-node et commencer le traitement
"""

from pyspark.sql import SparkSession
from pymongo import MongoClient
import pandas as pd
import os

print("="*60)
print("ğŸš€ CONNEXION MONGODB â†’ SPARK MULTI-NODE")
print("="*60)

# 1. Connexion au cluster Spark multi-node
print("\nâš¡ Connexion au cluster Spark...")
spark = SparkSession.builder \
    .appName("MongoDB_Spark_Cluster") \
    .master("spark://localhost:7077") \
    .config("spark.executor.memory", "2g") \
    .config("spark.executor.cores", "2") \
    .config("spark.sql.adaptive.enabled", "true") \
    .getOrCreate()

print(f"âœ… ConnectÃ© au cluster Spark")
print(f"   â€¢ Version: {spark.version}")
print(f"   â€¢ Master: {spark.sparkContext.master}")
print(f"   â€¢ Application ID: {spark.sparkContext.applicationId}")

# 2. Lecture depuis MongoDB
print("\nğŸ“‚ Connexion Ã  MongoDB...")
client = MongoClient('localhost', 27018, serverSelectionTimeoutMS=5000)
db = client['telecom_algerie']
collection = db['commentaires_bruts']

# VÃ©rifier le nombre de documents
total_mongo = collection.count_documents({})
print(f"ğŸ“Š MongoDB contient {total_mongo} documents")

# Charger les donnÃ©es
print("â³ Chargement des donnÃ©es depuis MongoDB...")
data = list(collection.find())
print(f"âœ… {len(data)} documents rÃ©cupÃ©rÃ©s")

# 3. Conversion en pandas DataFrame
print("\nğŸ¼ Conversion en pandas...")
df_pandas = pd.DataFrame(data)

# Supprimer l'ID MongoDB
if '_id' in df_pandas.columns:
    df_pandas = df_pandas.drop('_id', axis=1)

print(f"âœ… pandas DataFrame: {len(df_pandas)} lignes")
print(f"ğŸ“‹ Colonnes disponibles: {list(df_pandas.columns)}")

# 4. Conversion en Spark DataFrame (distribuÃ© sur le cluster)
print("\nğŸ”„ Conversion en Spark DataFrame...")
df_spark = spark.createDataFrame(df_pandas)
print(f"âœ… Spark DataFrame: {df_spark.count()} lignes")
print(f"âœ… Les donnÃ©es sont distribuÃ©es sur le cluster !")

# 5. Afficher un aperÃ§u
print("\nğŸ“Š AperÃ§u des 5 premiÃ¨res lignes:")
df_spark.show(5, truncate=50)

# 6. PremiÃ¨res analyses (dÃ©jÃ  distribuÃ©es !)
from pyspark.sql.functions import spark_partition_id, udf
from pyspark.sql.types import StringType
import socket

# 1. On crÃ©e une fonction qui rÃ©cupÃ¨re le nom du conteneur
def get_hostname():
    return socket.gethostname()

# 2. On enregistre cette fonction pour que Spark puisse l'utiliser sur les Workers
udf_get_hostname = udf(get_hostname, StringType())

print("\nğŸ” VÃ©rification de l'identitÃ© du travailleur (Worker ID)...")

# 3. On ajoute une colonne qui montre quel worker traite quelle ligne
df_verification = df_spark.withColumn("worker_name", udf_get_hostname()) \
                          .withColumn("partition_id", spark_partition_id())

# 4. On affiche les rÃ©sultats
df_verification.select("source", "worker_name", "partition_id").show(10)

# 5. On compte combien de lignes chaque worker a traitÃ©
print("ğŸ“Š RÃ©partition du travail par Worker :")
df_verification.groupBy("worker_name").count().show()

# 7. Sauvegarder en Parquet (dans /tmp d'abord)
print("\nğŸ’¾ Sauvegarde des donnÃ©es...")
tmp_path = "/tmp/donnees_mongodb.parquet"

# Supprimer l'ancien dossier s'il existe
import shutil
if os.path.exists(tmp_path):
    shutil.rmtree(tmp_path)

# Sauvegarder avec Spark (une seule partition pour Ã©viter les problÃ¨mes)
df_spark.coalesce(1).write.mode("overwrite").parquet(tmp_path)
print(f"âœ… DonnÃ©es sauvegardÃ©es dans: {tmp_path}")

# Copier vers le dossier du projet
final_path = "/home/mouna/projet_telecom/donnees/transformees/donnees_mongodb.parquet"
if os.path.exists(final_path):
    shutil.rmtree(final_path)

# Copier avec les permissions
os.system(f"cp -r {tmp_path} {final_path}")
os.system(f"chmod -R 755 {final_path}")
print(f"âœ… DonnÃ©es copiÃ©es vers: {final_path}")

print("\n" + "="*60)
print("ğŸ‰ CONNEXION RÃ‰USSIE !")
print("="*60)


spark.stop()
print("\nâœ… Session Spark terminÃ©e")

