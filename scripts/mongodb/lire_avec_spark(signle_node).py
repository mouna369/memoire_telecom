#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script pour lire les donnÃ©es MongoDB avec Spark
"""

from pyspark.sql import SparkSession
from pymongo import MongoClient
import pandas as pd
import time

print("="*60)
print("ğŸš€ CONNEXION SPARK â†’ MONGODB")
print("="*60)

# 1. RÃ©cupÃ©rer les donnÃ©es de MongoDB
print("\nğŸ“‚ RÃ©cupÃ©ration depuis MongoDB...")
try:
    client = MongoClient('localhost', 27017, serverSelectionTimeoutMS=5000)
    client.admin.command('ping')
    db = client['telecom_algerie']
    collection = db['commentaires_bruts']
    
    # Compter d'abord
    total = collection.count_documents({})
    print(f"ğŸ“Š MongoDB contient {total} commentaires")
    
    # RÃ©cupÃ©rer les donnÃ©es (par lots pour Ã©viter la mÃ©moire)
    print("â³ Chargement des donnÃ©es...")
    data = list(collection.find())
    print(f"âœ… {len(data)} documents rÃ©cupÃ©rÃ©s")
    
except Exception as e:
    print(f"âŒ Erreur de connexion MongoDB: {e}")
    exit(1)

# 2. Convertir en pandas DataFrame
print("\nğŸ¼ Conversion en pandas...")
df_pandas = pd.DataFrame(data)

# Enlever l'ID MongoDB (non nÃ©cessaire pour Spark)
if '_id' in df_pandas.columns:
    df_pandas = df_pandas.drop('_id', axis=1)

print(f"âœ… pandas DataFrame: {len(df_pandas)} lignes")
print(f"ğŸ“‹ Colonnes: {list(df_pandas.columns)}")

# 3. Initialiser Spark
print("\nâš¡ DÃ©marrage de Spark...")
spark = SparkSession.builder \
    .appName("MongoDB_vers_Spark") \
    .master("local[*]") \
    .config("spark.driver.memory", "4g") \
    .getOrCreate()

print(f"âœ… Spark dÃ©marrÃ© (version: {spark.version})")

# 4. Convertir en Spark DataFrame
print("\nğŸ”„ Conversion en Spark DataFrame...")
df_spark = spark.createDataFrame(df_pandas)
print(f"âœ… Spark DataFrame: {df_spark.count()} lignes")

# 5. Afficher un aperÃ§u
print("\nğŸ“Š AperÃ§u des 5 premiÃ¨res lignes:")
df_spark.show(5, truncate=50)

# 6. Statistiques rapides
print("\nğŸ“ˆ Statistiques:")
print(f"   â€¢ Commentaires Facebook: {df_spark.filter(df_spark.source == 'Facebook').count()}")
print(f"   â€¢ Commentaires Twitter: {df_spark.filter(df_spark.source == 'Twitter').count()}")
print(f"   â€¢ Commentaires avec date: {df_spark.filter(df_spark.date != '').count()}")

# 7. Sauvegarder en Parquet (pour la suite)
print("\nğŸ’¾ Sauvegarde en Parquet...")
output_path = "donnees/transformees/depuis_mongodb.parquet"
df_spark.write.mode("overwrite").parquet(output_path)
print(f"âœ… DonnÃ©es sauvegardÃ©es dans: {output_path}")

print("\n" + "="*60)
print("ğŸ‰ SUCCÃˆS ! Les donnÃ©es sont prÃªtes dans Spark !")
print("="*60)

# 8. Petit test de requÃªte Spark
print("\nğŸ” Test: RÃ©partition par source:")
df_spark.groupBy("source").count().show()

spark.stop()
print("\nâœ… Spark arrÃªtÃ©")