

# #!/usr/bin/env python3
# # -*- coding: utf-8 -*-

# # scripts/nettoyage/01_supprimer_urls_multinode.py - VERSION AVEC MESURE DE TEMPS

# from pyspark.sql import SparkSession
# from pyspark.sql.functions import col, udf, spark_partition_id
# from pyspark.sql.types import StringType, IntegerType
# import re
# from pymongo import MongoClient
# from datetime import datetime
# import os
# import socket
# import pandas as pd
# import time  # ğŸ‘ˆ POUR MESURER LE TEMPS

# def supprimer_urls(texte):
#     """Supprime les URLs d'un texte - Version amÃ©liorÃ©e"""
#     if texte is None or not isinstance(texte, str):
#         return texte
    
#     # Patterns amÃ©liorÃ©s pour dÃ©tecter tous les types d'URLs
#     patterns = [
#         r'https?://\S+',           # URLs complÃ¨tes
#         r'www\.\S+',                # www.example.com
#         r'https?://(?:\s|$)',       # https:// seul suivi d'espace ou fin
#         r'https?://$',              # https:// en fin de chaÃ®ne
#         r'\bhttps?://\b',           # https:// comme mot isolÃ©
#         r'http://(?:\s|$)',         # http:// seul
#         r'http://$'                 # http:// en fin de chaÃ®ne
#     ]
    
#     texte_propre = texte
#     for pattern in patterns:
#         texte_propre = re.sub(pattern, '', texte_propre, flags=re.IGNORECASE)
    
#     # Supprimer les espaces multiples
#     texte_propre = re.sub(r'\s+', ' ', texte_propre).strip()
#     return texte_propre if texte_propre else None

# def supprimer_at(texte):
#     """Supprime les caractÃ¨res @ d'un texte"""
#     if texte is None or not isinstance(texte, str):
#         return texte
    
#     # Supprimer tous les @
#     texte_propre = re.sub(r'@', '', texte)
    
#     # Supprimer les espaces multiples
#     texte_propre = re.sub(r'\s+', ' ', texte_propre).strip()
#     return texte_propre if texte_propre else None

# def detecter_urls(texte):
#     """DÃ©tecte si un texte contient des URLs"""
#     if texte is None or not isinstance(texte, str):
#         return 0
    
#     patterns = [
#         r'https?://',
#         r'www\.',
#         r'https?://(?:\s|$)',
#         r'https?://$'
#     ]
    
#     for pattern in patterns:
#         if re.search(pattern, texte, re.IGNORECASE):
#             return 1
#     return 0

# def detecter_at(texte):
#     """DÃ©tecte si un texte contient des @"""
#     if texte is None or not isinstance(texte, str):
#         return 0
    
#     return 1 if re.search(r'@', texte) else 0

# def compter_at(texte):
#     """Compte le nombre de @ dans un texte"""
#     if texte is None or not isinstance(texte, str):
#         return 0
    
#     return len(re.findall(r'@', texte))

# def compter_urls(texte):
#     """Compte le nombre d'URLs dans un texte"""
#     if texte is None or not isinstance(texte, str):
#         return 0
    
#     pattern = r'https?://\S+|www\.\S+|https?://(?:\s|$)|https?://$'
#     return len(re.findall(pattern, texte, re.IGNORECASE))

# def get_worker_name():
#     """Retourne le nom du worker"""
#     return socket.gethostname()

# # ğŸ“Š DÃ‰BUT DU CHRONOMÃˆTRAGE GLOBAL
# temps_debut_global = time.time()

# print("="*70)
# print("ğŸ” Ã‰TAPE 1 : SUPPRESSION DES URLS ET DES @ - MODE MULTI-NODE")
# print("="*70)

# # 1. CONNEXION Ã€ MONGODB
# print("\nğŸ“‚ Connexion Ã  MongoDB...")
# try:
#     client = MongoClient('localhost', 27018)
#     db = client['telecom_algerie']
#     collection_source = db['commentaires_bruts']
#     total_docs = collection_source.count_documents({})
#     print(f"âœ… Connexion MongoDB rÃ©ussie")
#     print(f"ğŸ“Š Collection source: {total_docs} documents")
# except Exception as e:
#     print(f"âŒ Erreur de connexion MongoDB: {e}")
#     exit(1)

# # 2. CONNEXION AU CLUSTER SPARK
# print("\nâš¡ Connexion au cluster Spark multi-node...")
# temps_debut_spark = time.time()

# spark = SparkSession.builder \
#     .appName("Suppression_URLs_MultiNode") \
#     .master("spark://spark-master:7077") \
#     .config("spark.executor.memory", "2g") \
#     .config("spark.executor.cores", "12") \
#     .getOrCreate()

# temps_fin_spark = time.time()
# print(f"âœ… Cluster Spark multi-node connectÃ© en {temps_fin_spark - temps_debut_spark:.2f} secondes")

# # 3. CHARGER LES DONNÃ‰ES AVEC PYMONGO
# print("\nğŸ“¥ Chargement des donnÃ©es avec PyMongo...")
# temps_debut_chargement = time.time()

# # Charger tous les documents
# print("   RÃ©cupÃ©ration des documents...")
# data = list(collection_source.find({}))
# print(f"   ğŸ“Š {len(data)} documents chargÃ©s")

# # Convertir les ObjectId en string
# print("   ğŸ”„ Conversion des ObjectId...")
# for doc in data:
#     doc['_id'] = str(doc['_id'])

# # CrÃ©er DataFrame Spark
# print("   ğŸ“Š CrÃ©ation du DataFrame Spark...")
# df_spark = spark.createDataFrame(data)
# total_lignes = df_spark.count()

# temps_fin_chargement = time.time()
# print(f"âœ… {total_lignes} documents chargÃ©s dans Spark en {temps_fin_chargement - temps_debut_chargement:.2f} secondes")

# # 4. IDENTIFIER LES WORKERS
# print("\nğŸ” RÃ‰PARTITION SUR LES WORKERS:")

# worker_udf = udf(get_worker_name, StringType())

# df_with_workers = df_spark \
#     .withColumn("partition_id", spark_partition_id()) \
#     .withColumn("worker_name", worker_udf())

# print("   Distribution des donnÃ©es:")
# df_with_workers.groupBy("worker_name", "partition_id").count().show()

# # 5. ENREGISTRER LES UDF
# print("\nğŸ”„ Enregistrement des fonctions...")
# supprimer_urls_udf = udf(supprimer_urls, StringType())
# supprimer_at_udf = udf(supprimer_at, StringType())
# detecter_urls_udf = udf(detecter_urls, IntegerType())
# detecter_at_udf = udf(detecter_at, IntegerType())
# compter_urls_udf = udf(compter_urls, IntegerType())
# compter_at_udf = udf(compter_at, IntegerType())

# # 6. ANALYSE AVANT NETTOYAGE
# print("\nğŸ” ANALYSE : Recherche des URLs et des @...")
# temps_debut_analyse = time.time()

# df_analyse = df_with_workers \
#     .withColumn("urls_avant", detecter_urls_udf(col("Commentaire_Client"))) \
#     .withColumn("nb_urls_avant", compter_urls_udf(col("Commentaire_Client"))) \
#     .withColumn("at_avant", detecter_at_udf(col("Commentaire_Client"))) \
#     .withColumn("nb_at_avant", compter_at_udf(col("Commentaire_Client")))

# total = df_analyse.count()
# avec_urls_avant = df_analyse.filter(col("urls_avant") == 1).count()
# total_urls = df_analyse.agg({"nb_urls_avant": "sum"}).collect()[0][0] or 0
# avec_at_avant = df_analyse.filter(col("at_avant") == 1).count()
# total_at = df_analyse.agg({"nb_at_avant": "sum"}).collect()[0][0] or 0

# temps_fin_analyse = time.time()
# print(f"\nğŸ“Š STATISTIQUES AVANT NETTOYAGE (analyse en {temps_fin_analyse - temps_debut_analyse:.2f}s):")
# print(f"   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
# print(f"   â”‚ Total documents        : {total:<15} â”‚")
# print(f"   â”‚ Documents avec URLs    : {avec_urls_avant:<15} â”‚")
# print(f"   â”‚ URLs dÃ©tectÃ©es         : {total_urls:<15} â”‚")
# print(f"   â”‚ Documents avec @       : {avec_at_avant:<15} â”‚")
# print(f"   â”‚ @ dÃ©tectÃ©s             : {total_at:<15} â”‚")
# print(f"   â”‚ Pourcentage URLs       : {(avec_urls_avant/total*100):<15.2f}% â”‚")
# print(f"   â”‚ Pourcentage @          : {(avec_at_avant/total*100):<15.2f}% â”‚")
# print(f"   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")

# # 7. NETTOYAGE
# print("\nğŸ§¹ SUPPRESSION DES URLS ET DES @ EN COURS...")
# temps_debut_nettoyage = time.time()

# # Appliquer d'abord la suppression des URLs, puis la suppression des @
# df_nettoye = df_analyse \
#     .withColumn("Commentaire_Client_sans_urls", supprimer_urls_udf(col("Commentaire_Client"))) \
#     .withColumn("commentaire_moderateur_sans_urls", supprimer_urls_udf(col("commentaire_moderateur"))) \
#     .withColumn("Commentaire_Client_propre", supprimer_at_udf(col("Commentaire_Client_sans_urls"))) \
#     .withColumn("commentaire_moderateur_propre", supprimer_at_udf(col("commentaire_moderateur_sans_urls"))) \
#     .withColumn("urls_apres", detecter_urls_udf(col("Commentaire_Client_propre"))) \
#     .withColumn("at_apres", detecter_at_udf(col("Commentaire_Client_propre")))

# # Forcer l'exÃ©cution des transformations
# df_nettoye.cache().count()

# temps_fin_nettoyage = time.time()
# print(f"âœ… Nettoyage terminÃ© en {temps_fin_nettoyage - temps_debut_nettoyage:.2f} secondes")

# # 8. STATISTIQUES APRÃˆS NETTOYAGE
# avec_urls_apres = df_nettoye.filter(col("urls_apres") == 1).count()
# avec_at_apres = df_nettoye.filter(col("at_apres") == 1).count()
# supprimees_urls = avec_urls_avant - avec_urls_apres
# supprimees_at = avec_at_avant - avec_at_apres
# taux_urls = (supprimees_urls / avec_urls_avant * 100) if avec_urls_avant > 0 else 0
# taux_at = (supprimees_at / avec_at_avant * 100) if avec_at_avant > 0 else 0

# print(f"\nğŸ“Š STATISTIQUES APRÃˆS NETTOYAGE:")
# print(f"   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
# print(f"   â”‚ URLs avant            : {avec_urls_avant:<15} â”‚")
# print(f"   â”‚ URLs aprÃ¨s            : {avec_urls_apres:<15} â”‚")
# print(f"   â”‚ URLs supprimÃ©es       : {supprimees_urls:<15} â”‚")
# print(f"   â”‚ Taux succÃ¨s URLs      : {taux_urls:<15.2f}% â”‚")
# print(f"   â”‚ @ avant               : {avec_at_avant:<15} â”‚")
# print(f"   â”‚ @ aprÃ¨s               : {avec_at_apres:<15} â”‚")
# print(f"   â”‚ @ supprimÃ©s           : {supprimees_at:<15} â”‚")
# print(f"   â”‚ Taux succÃ¨s @         : {taux_at:<15.2f}% â”‚")
# print(f"   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")

# # 9. STATISTIQUES PAR WORKER
# print("\nğŸ“Š PERFORMANCE PAR WORKER:")
# worker_perf = df_nettoye.groupBy("worker_name").agg(
#     {"urls_avant": "sum", 
#      "urls_apres": "sum",
#      "at_avant": "sum",
#      "at_apres": "sum",
#      "Commentaire_Client": "count"}
# ).withColumnRenamed("sum(urls_avant)", "urls_trouvees") \
#  .withColumnRenamed("sum(urls_apres)", "urls_restantes") \
#  .withColumnRenamed("sum(at_avant)", "at_trouves") \
#  .withColumnRenamed("sum(at_apres)", "at_restants") \
#  .withColumnRenamed("count(Commentaire_Client)", "documents")

# worker_perf.show()

# # 10. PRÃ‰PARATION POUR MONGODB
# print("\nğŸ’¾ PRÃ‰PARATION POUR SAUVEGARDE...")
# temps_debut_preparation = time.time()

# df_final = df_nettoye.select(
#     "_id",
#     col("Commentaire_Client_propre").alias("Commentaire_Client"),
#     col("commentaire_moderateur_propre").alias("commentaire_moderateur"),
#     "date",
#     "source",
#     "moderateur",
#     "metadata",
#     "statut"
# )

# temps_fin_preparation = time.time()
# print(f"âœ… PrÃ©paration terminÃ©e en {temps_fin_preparation - temps_debut_preparation:.2f} secondes")

# # 11. SAUVEGARDE DANS MONGODB
# print("\nğŸ“ SAUVEGARDE DANS MONGODB...")
# temps_debut_sauvegarde = time.time()

# # Convertir en Pandas
# print("   ğŸ”„ Conversion en Pandas...")
# df_pandas = df_final.toPandas()
# print(f"   âœ… {len(df_pandas)} lignes converties")

# # Collection destination
# collection_dest = db['commentaires_sans_urls_multinode2']
# collection_dest.delete_many({})
# print("   ğŸ§¹ Collection destination vidÃ©e")

# # InsÃ©rer par lots
# print("   ğŸ“¥ Insertion par lots...")
# batch_size = 500
# total_batches = (len(df_pandas) + batch_size - 1) // batch_size

# for i in range(0, len(df_pandas), batch_size):
#     batch_num = i//batch_size + 1
#     batch = df_pandas.iloc[i:i+batch_size].to_dict('records')
    
#     # Convertir les NaN en None pour MongoDB
#     for doc in batch:
#         for key, value in doc.items():
#             if pd.isna(value):
#                 doc[key] = None
    
#     collection_dest.insert_many(batch)
#     print(f"   âœ“ Lot {batch_num}/{total_batches}: {len(batch)} documents")

# temps_fin_sauvegarde = time.time()
# print(f"\nâœ… {len(df_pandas)} documents sauvegardÃ©s dans 'commentaires_sans_urls_multinode2'")
# print(f"   â±ï¸  Temps de sauvegarde: {temps_fin_sauvegarde - temps_debut_sauvegarde:.2f} secondes")

# # 12. VÃ‰RIFICATION FINALE
# print("\nğŸ” VÃ‰RIFICATION FINALE...")
# temps_debut_verification = time.time()

# # VÃ©rifier avec diffÃ©rents patterns
# patterns_verif_urls = [
#     r'https?://\S+',
#     r'www\.\S+',
#     r'https?://(?:\s|$)',
#     r'https?://$'
# ]

# print("\n   VÃ©rification URLs pattern par pattern:")
# for pattern in patterns_verif_urls:
#     count = collection_dest.count_documents({
#         "Commentaire_Client": {"$regex": pattern, "$options": "i"}
#     })
#     print(f"   â€¢ {pattern[:20]}...: {count} documents")

# # VÃ©rification des @
# count_at = collection_dest.count_documents({
#     "Commentaire_Client": {"$regex": "@", "$options": "i"}
# })
# print(f"\n   â€¢ VÃ©rification @: {count_at} documents avec @")

# # VÃ©rification globale
# urls_restantes = collection_dest.count_documents({
#     "Commentaire_Client": {"$regex": "https?://|www\.", "$options": "i"}
# })

# temps_fin_verification = time.time()
# print(f"\nğŸ“Š RÃ‰SULTAT FINAL (vÃ©rification en {temps_fin_verification - temps_debut_verification:.2f}s):")
# print(f"   â€¢ Documents avec URLs restantes: {urls_restantes}")
# print(f"   â€¢ Documents avec @ restants: {count_at}")
# if urls_restantes == 0 and count_at == 0:
#     print("   âœ… SUCCÃˆS : Toutes les URLs et tous les @ ont Ã©tÃ© supprimÃ©s !")
# else:
#     print(f"   âš ï¸ ATTENTION : {urls_restantes} URLs restantes, {count_at} @ restants")

# # ğŸ FIN DU CHRONOMÃˆTRAGE GLOBAL
# temps_fin_global = time.time()
# temps_total = temps_fin_global - temps_debut_global

# # 13. RAPPORT AVEC TEMPS D'EXÃ‰CUTION
# print("\nğŸ“„ CRÃ‰ATION DU RAPPORT...")

# rapport = f"""
# {"="*70}
# RAPPORT DE SUPPRESSION DES URLS ET DES @ - MODE MULTI-NODE
# {"="*70}

# Date d'exÃ©cution : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
# Mode : Multi-node Spark Cluster

# â±ï¸  TEMPS D'EXÃ‰CUTION:
#    â€¢ Connexion Spark        : {temps_fin_spark - temps_debut_spark:.2f}s
#    â€¢ Chargement donnÃ©es     : {temps_fin_chargement - temps_debut_chargement:.2f}s
#    â€¢ Analyse des donnÃ©es    : {temps_fin_analyse - temps_debut_analyse:.2f}s
#    â€¢ Nettoyage              : {temps_fin_nettoyage - temps_debut_nettoyage:.2f}s
#    â€¢ PrÃ©paration DataFrame  : {temps_fin_preparation - temps_debut_preparation:.2f}s
#    â€¢ Sauvegarde MongoDB     : {temps_fin_sauvegarde - temps_debut_sauvegarde:.2f}s
#    â€¢ VÃ©rification           : {temps_fin_verification - temps_debut_verification:.2f}s
#    â€¢ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#    â€¢ TEMPS TOTAL            : {temps_total:.2f}s
#    â€¢ Documents par seconde  : {total / temps_total:.2f} doc/s

# ğŸ“Š STATISTIQUES GLOBALES:
#    â€¢ Total documents traitÃ©s    : {total}
   
#    ğŸ“ URLs:
#    â€¢ Documents avec URLs (avant): {avec_urls_avant}
#    â€¢ URLs dÃ©tectÃ©es (avant)     : {total_urls}
#    â€¢ Documents avec URLs (aprÃ¨s): {avec_urls_apres}
#    â€¢ URLs supprimÃ©es            : {supprimees_urls}
#    â€¢ Taux de succÃ¨s URLs        : {taux_urls:.2f}%
   
#    ğŸ“ @ (arobase):
#    â€¢ Documents avec @ (avant)   : {avec_at_avant}
#    â€¢ @ dÃ©tectÃ©s (avant)         : {total_at}
#    â€¢ Documents avec @ (aprÃ¨s)   : {avec_at_apres}
#    â€¢ @ supprimÃ©s                : {supprimees_at}
#    â€¢ Taux de succÃ¨s @           : {taux_at:.2f}%

# ğŸ“ STOCKAGE:
#    â€¢ Collection source      : telecom_algerie.commentaires_bruts
#    â€¢ Collection destination : telecom_algerie.commentaires_sans_urls_multinode2
#    â€¢ Documents sauvegardÃ©s  : {len(df_pandas)}

# ğŸ” VÃ‰RIFICATION FINALE:
#    â€¢ URLs restantes dÃ©tectÃ©es : {urls_restantes}
#    â€¢ @ restants dÃ©tectÃ©s      : {count_at}
#    â€¢ Statut : {"âœ… SUCCÃˆS" if (urls_restantes == 0 and count_at == 0) else "âš ï¸ Ã‰CHEC"}

# âš¡ DISTRIBUTION:
#    â€¢ Workers utilisÃ©s : {df_with_workers.select("worker_name").distinct().count()}
# """

# # Sauvegarder le rapport
# os.makedirs("donnees/resultats", exist_ok=True)
# rapport_path = "donnees/resultats/rapport_urls_multinode2.txt"
# with open(rapport_path, "w", encoding="utf-8") as f:
#     f.write(rapport)
# print(f"âœ… Rapport sauvegardÃ©: {rapport_path}")

# # 14. RÃ‰SUMÃ‰ FINAL AVEC TEMPS
# print("\n" + "="*70)
# print("ğŸ“Š RÃ‰SUMÃ‰ FINAL - MODE MULTI-NODE")
# print("="*70)
# print(f"ğŸ“¥ Documents traitÃ©s    : {total}")
# print(f"\nğŸ“ URLs:")
# print(f"   â€¢ DÃ©tectÃ©es : {total_urls}")
# print(f"   â€¢ SupprimÃ©es: {supprimees_urls}")
# print(f"   â€¢ Taux      : {taux_urls:.2f}%")
# print(f"\nğŸ“ @ (arobase):")
# print(f"   â€¢ DÃ©tectÃ©s  : {total_at}")
# print(f"   â€¢ SupprimÃ©s : {supprimees_at}")
# print(f"   â€¢ Taux      : {taux_at:.2f}%")
# print(f"\nâ±ï¸  TEMPS D'EXÃ‰CUTION:")
# print(f"   â€¢ Chargement : {temps_fin_chargement - temps_debut_chargement:.2f}s")
# print(f"   â€¢ Nettoyage  : {temps_fin_nettoyage - temps_debut_nettoyage:.2f}s")
# print(f"   â€¢ Sauvegarde : {temps_fin_sauvegarde - temps_debut_sauvegarde:.2f}s")
# print(f"   â€¢ TOTAL      : {temps_total:.2f}s")
# print(f"   â€¢ Vitesse    : {total / temps_total:.2f} docs/s")
# print(f"\nğŸ“ Collection MongoDB:")
# print(f"   â€¢ telecom_algerie.commentaires_sans_urls_multinode2")
# print("="*70)

# print("\nğŸ‰ SUPPRESSION DES URLS ET DES @ TERMINÃ‰E EN MODE MULTI-NODE !")

# # Fermer les connexions
# spark.stop()
# client.close()
# print("\nğŸ”Œ Connexions fermÃ©es")

#!/usr/bin/env python3
# -*- coding: utf-8 -*-

# scripts/nettoyage/01_supprimer_urls_multinode_v2.py
# VERSION VRAIMENT DISTRIBUÃ‰E â€” mapPartitions
# Spark 4.1.1 / Scala 2.13 / Java 21
# Chaque Worker lit ET Ã©crit MongoDB directement

from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col, regexp_replace, spark_partition_id,
    trim, when
)
from pyspark.sql.functions import sum as spark_sum, count as spark_count
from pymongo import MongoClient, InsertOne
from pymongo.errors import BulkWriteError
from datetime import datetime
import os
import time
import math

# ============================================================
# CONFIGURATION CENTRALISÃ‰E
# ============================================================
# âœ… Le Driver (WSL) accÃ¨de Ã  MongoDB via localhost:27018
MONGO_URI_DRIVER = "mongodb://localhost:27018/"

# âœ… Les Workers (Docker) accÃ¨dent Ã  MongoDB via le nom du conteneur
#    sur le rÃ©seau projet_telecom_spark_network
MONGO_URI_WORKERS = "mongodb://mongodb_pfe:27017/"

DB_NAME             = "telecom_algerie"
COLLECTION_SOURCE   = "commentaires_bruts"
COLLECTION_DEST     = "commentaires_sans_urls_arobase"
BATCH_SIZE          = 1000
SPARK_MASTER        = "spark://spark-master:7077"
RAPPORT_PATH        = "Rapports/rapport_urls_arobase.txt"

# Patterns Regex
PATTERN_URLS        = r'https?://\S*|www\.\S+'
PATTERN_AT          = r'@'
PATTERN_ESPACES     = r'\s+'
PATTERN_DETECTION   = r'https?://|www\.'

# ============================================================
# FONCTIONS DISTRIBUÃ‰ES (s'exÃ©cutent sur les Workers)
# ============================================================

def nettoyer_et_ecrire_partition(partition):
    """
    âœ… Cette fonction s'exÃ©cute sur chaque Worker indÃ©pendamment.
    
    Chaque Worker :
    1. Nettoie sa portion de donnÃ©es (regex)
    2. Se connecte directement Ã  MongoDB
    3. Ã‰crit ses donnÃ©es nettoyÃ©es dans MongoDB
    Sans jamais passer par le Driver !
    """
    import re
    from pymongo import MongoClient, InsertOne
    from pymongo.errors import BulkWriteError

    # Patterns compilÃ©s une seule fois par partition (performance)
    re_urls    = re.compile(r'https?://\S+|www\.\S+', re.IGNORECASE)
    re_at      = re.compile(r'@')
    re_espaces = re.compile(r'\s+')

    def nettoyer_texte(texte):
        if not texte or not isinstance(texte, str):
            return texte
        texte = re_urls.sub('', texte)
        texte = re_at.sub('', texte)
        texte = re_espaces.sub(' ', texte).strip()
        return texte if texte else None

    # Connexion MongoDB depuis le Worker
    # âœ… Utilise mongodb_pfe:27017 (rÃ©seau Docker interne)
    try:
        client = MongoClient(
            "mongodb://mongodb_pfe:27017/",
            serverSelectionTimeoutMS=5000
        )
        db = client["telecom_algerie"]
        collection = db["commentaires_sans_urls_arobase"]
    except Exception as e:
        # Si connexion Ã©choue, on yield les erreurs pour le Driver
        yield {"_erreur": str(e), "partition": "connexion_failed"}
        return

    batch = []
    docs_traites = 0

    for row in partition:
        # Nettoyage des champs texte
        commentaire_propre = nettoyer_texte(row.get("Commentaire_Client"))
        moderateur_propre  = nettoyer_texte(row.get("commentaire_moderateur"))

        doc = {
            "_id"                    : row.get("_id"),
            "Commentaire_Client"     : commentaire_propre,
            "commentaire_moderateur" : moderateur_propre,
            "date"                   : row.get("date"),
            "source"                 : row.get("source"),
            "moderateur"             : row.get("moderateur"),
            "metadata"               : row.get("metadata"),
            "statut"                 : row.get("statut"),
        }

        # Nettoyer les None/NaN
        doc_propre = {k: (None if v != v else v) for k, v in doc.items()}
        batch.append(InsertOne(doc_propre))
        docs_traites += 1

        # Ã‰criture par lots depuis le Worker
        if len(batch) >= 1000:
            try:
                collection.bulk_write(batch, ordered=False)
            except BulkWriteError:
                pass  # Ignorer les doublons Ã©ventuels
            batch = []

    # Ã‰crire le dernier lot
    if batch:
        try:
            collection.bulk_write(batch, ordered=False)
        except BulkWriteError:
            pass

    client.close()

    # âœ… Retourner les stats de cette partition au Driver
    yield {"docs_traites": docs_traites, "statut": "ok"}


def lire_partition_depuis_mongo(partition_info):
    """
    âœ… Chaque Worker lit SA portion depuis MongoDB directement.
    
    partition_info contient : (skip, limit) â€” la plage de documents
    que ce Worker doit traiter.
    """
    from pymongo import MongoClient

    for item in partition_info:
        skip  = item["skip"]
        limit = item["limit"]

        client = MongoClient(
            "mongodb://mongodb_pfe:27017/",
            serverSelectionTimeoutMS=5000
        )
        db = client["telecom_algerie"]
        collection = db["commentaires_bruts"]

        curseur = collection.find(
            {},
            {"_id": 1, "Commentaire_Client": 1,
             "commentaire_moderateur": 1, "date": 1,
             "source": 1, "moderateur": 1,
             "metadata": 1, "statut": 1}
        ).skip(skip).limit(limit)

        for doc in curseur:
            doc["_id"] = str(doc["_id"])
            yield doc

        client.close()


# ============================================================
# DÃ‰BUT DU PIPELINE
# ============================================================
temps_debut_global = time.time()

print("="*70)
print("ğŸ” SUPPRESSION DES URLS ET @ â€” VERSION VRAIMENT DISTRIBUÃ‰E")
print("   Spark 4.1.1 | mapPartitions | Workers â†’ MongoDB direct")
print("="*70)

# ============================================================
# 1. CONNEXION MONGODB DRIVER (pour prÃ©parer le job)
# ============================================================
print("\nğŸ“‚ Connexion MongoDB (Driver)...")
try:
    client_driver = MongoClient(MONGO_URI_DRIVER)
    db_driver     = client_driver[DB_NAME]
    coll_source   = db_driver[COLLECTION_SOURCE]
    total_docs    = coll_source.count_documents({})
    print(f"âœ… MongoDB connectÃ© â€” {total_docs} documents dans la source")
except Exception as e:
    print(f"âŒ Erreur MongoDB : {e}")
    exit(1)

# ============================================================
# 2. CONNEXION SPARK
# ============================================================
print("\nâš¡ Connexion au cluster Spark...")
temps_debut_spark = time.time()

spark = SparkSession.builder \
    .appName("Suppression_URLs_MapPartitions") \
    .master(SPARK_MASTER) \
    .config("spark.executor.memory", "2g") \
    .config("spark.executor.cores", "2") \
    .config("spark.sql.shuffle.partitions", "4") \
    .getOrCreate()

spark.sparkContext.setLogLevel("WARN")
temps_fin_spark = time.time()
print(f"âœ… Spark connectÃ© en {temps_fin_spark - temps_debut_spark:.2f}s")

# ============================================================
# 3. DISTRIBUER LA LECTURE ENTRE LES WORKERS
#    Le Driver crÃ©e juste un RDD de "plages" (skip/limit)
#    Chaque Worker lit sa plage directement depuis MongoDB
# ============================================================
print("\nğŸ“¥ LECTURE DISTRIBUÃ‰E â€” Chaque Worker lit sa portion...")
temps_debut_chargement = time.time()

NB_WORKERS   = 2  # Adapte selon ton nombre de workers
docs_par_worker = math.ceil(total_docs / NB_WORKERS)

# CrÃ©er les plages pour chaque Worker
plages = [
    {"skip": i * docs_par_worker,
     "limit": min(docs_par_worker, total_docs - i * docs_par_worker)}
    for i in range(NB_WORKERS)
]

print(f"   Distribution :")
for idx, p in enumerate(plages):
    print(f"   â€¢ Worker {idx+1} : skip={p['skip']}, limit={p['limit']} docs")

# âœ… Chaque Worker lit sa portion depuis MongoDB via mapPartitions
rdd_data = spark.sparkContext \
    .parallelize(plages, NB_WORKERS) \
    .mapPartitions(lire_partition_depuis_mongo)

# Convertir en DataFrame Spark
df_spark = spark.read.json(rdd_data.map(
    lambda d: __import__('json').dumps(
        {k: str(v) if not isinstance(v, (str, int, float, bool, type(None))) else v
         for k, v in d.items()}
    )
))

total_lignes = df_spark.count()
temps_fin_chargement = time.time()

print(f"âœ… {total_lignes} documents chargÃ©s en {temps_fin_chargement - temps_debut_chargement:.2f}s")
print(f"   Partitions Spark : {df_spark.rdd.getNumPartitions()}")

# ============================================================
# 4. ANALYSE AVANT NETTOYAGE (fonctions natives Spark)
# ============================================================
print("\nğŸ” ANALYSE AVANT NETTOYAGE...")
temps_debut_analyse = time.time()

df_analyse = df_spark \
    .withColumn("a_url",
        when(col("Commentaire_Client").rlike(PATTERN_DETECTION), 1).otherwise(0)) \
    .withColumn("a_at",
        when(col("Commentaire_Client").rlike(PATTERN_AT), 1).otherwise(0))

stats_avant = df_analyse.agg(
    spark_count("*").alias("total"),
    spark_sum("a_url").alias("avec_urls"),
    spark_sum("a_at").alias("avec_at")
).collect()[0]

total        = stats_avant["total"]
avec_urls_av = int(stats_avant["avec_urls"] or 0)
avec_at_av   = int(stats_avant["avec_at"] or 0)

temps_fin_analyse = time.time()
print(f"\nğŸ“Š AVANT NETTOYAGE (en {temps_fin_analyse - temps_debut_analyse:.2f}s):")
print(f"   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
print(f"   â”‚ Total documents     : {total:<15} â”‚")
print(f"   â”‚ Avec URLs           : {avec_urls_av:<15} â”‚")
print(f"   â”‚ Avec @              : {avec_at_av:<15} â”‚")
print(f"   â”‚ % URLs              : {(avec_urls_av/total*100):<14.2f}% â”‚")
print(f"   â”‚ % @                 : {(avec_at_av/total*100):<14.2f}% â”‚")
print(f"   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")

# ============================================================
# 5. NETTOYAGE (fonctions natives Spark sur les Workers)
# ============================================================
print("\nğŸ§¹ NETTOYAGE SUR LES WORKERS...")
temps_debut_nettoyage = time.time()

df_nettoye = df_spark \
    .withColumn("Commentaire_Client",
        trim(regexp_replace(
            regexp_replace(col("Commentaire_Client"), PATTERN_URLS, ""),
            PATTERN_AT, ""))
    ) \
    .withColumn("Commentaire_Client",
        regexp_replace(col("Commentaire_Client"), PATTERN_ESPACES, " ")) \
    .withColumn("commentaire_moderateur",
        trim(regexp_replace(
            regexp_replace(col("commentaire_moderateur"), PATTERN_URLS, ""),
            PATTERN_AT, ""))
    ) \
    .withColumn("commentaire_moderateur",
        regexp_replace(col("commentaire_moderateur"), PATTERN_ESPACES, " "))

df_nettoye.cache()

# Stats aprÃ¨s nettoyage
df_apres = df_nettoye \
    .withColumn("urls_ap", when(col("Commentaire_Client").rlike(PATTERN_DETECTION), 1).otherwise(0)) \
    .withColumn("at_ap",   when(col("Commentaire_Client").rlike(PATTERN_AT), 1).otherwise(0))

stats_apres = df_apres.agg(
    spark_sum("urls_ap").alias("avec_urls"),
    spark_sum("at_ap").alias("avec_at")
).collect()[0]

avec_urls_ap = int(stats_apres["avec_urls"] or 0)
avec_at_ap   = int(stats_apres["avec_at"] or 0)

temps_fin_nettoyage = time.time()

taux_urls = ((avec_urls_av - avec_urls_ap) / avec_urls_av * 100) if avec_urls_av > 0 else 100.0
taux_at   = ((avec_at_av - avec_at_ap) / avec_at_av * 100) if avec_at_av > 0 else 100.0

print(f"âœ… Nettoyage terminÃ© en {temps_fin_nettoyage - temps_debut_nettoyage:.2f}s")
print(f"\nğŸ“Š APRÃˆS NETTOYAGE:")
print(f"   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
print(f"   â”‚ URLs supprimÃ©es     : {avec_urls_av - avec_urls_ap:<15} â”‚")
print(f"   â”‚ Taux URLs           : {taux_urls:<14.2f}% â”‚")
print(f"   â”‚ @ supprimÃ©s         : {avec_at_av - avec_at_ap:<15} â”‚")
print(f"   â”‚ Taux @              : {taux_at:<14.2f}% â”‚")
print(f"   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")

# ============================================================
# 6. Ã‰CRITURE DISTRIBUÃ‰E DANS MONGODB
#    âœ… Chaque Worker Ã©crit directement sa partition
#    Le Driver ne touche PAS les donnÃ©es
# ============================================================
print("\nğŸ’¾ Ã‰CRITURE DISTRIBUÃ‰E â€” Chaque Worker Ã©crit dans MongoDB...")
temps_debut_sauvegarde = time.time()

# Vider la collection destination depuis le Driver
coll_dest = db_driver[COLLECTION_DEST]
coll_dest.delete_many({})
print("   ğŸ§¹ Collection destination vidÃ©e")

# SÃ©lectionner les colonnes finales
df_final = df_nettoye.select(
    "_id", "Commentaire_Client", "commentaire_moderateur",
    "date", "source", "moderateur", "metadata", "statut"
)

# âœ… mapPartitions : chaque Worker Ã©crit sa portion directement
print("   ğŸ“¤ Workers en train d'Ã©crire dans MongoDB...")
rdd_stats = df_final.rdd \
    .map(lambda row: row.asDict()) \
    .mapPartitions(nettoyer_et_ecrire_partition)

# Collecter juste les stats (petits objets, pas les donnÃ©es)
stats_ecriture = rdd_stats.collect()

total_inseres = sum(
    s.get("docs_traites", 0)
    for s in stats_ecriture
    if s.get("statut") == "ok"
)
erreurs = [s for s in stats_ecriture if "_erreur" in s]

temps_fin_sauvegarde = time.time()

print(f"âœ… {total_inseres} documents Ã©crits en {temps_fin_sauvegarde - temps_debut_sauvegarde:.2f}s")

if erreurs:
    print(f"   âš ï¸  {len(erreurs)} erreur(s) dÃ©tectÃ©e(s):")
    for e in erreurs:
        print(f"      â€¢ {e.get('_erreur')}")

# ============================================================
# 7. VÃ‰RIFICATION FINALE (depuis le Driver)
# ============================================================
print("\nğŸ” VÃ‰RIFICATION FINALE...")
temps_debut_verif = time.time()

urls_restantes = coll_dest.count_documents({
    "Commentaire_Client": {"$regex": PATTERN_DETECTION, "$options": "i"}
})
at_restants = coll_dest.count_documents({
    "Commentaire_Client": {"$regex": "@"}
})
total_en_dest = coll_dest.count_documents({})

temps_fin_verif = time.time()

print(f"   â€¢ Documents en destination : {total_en_dest}")
print(f"   â€¢ URLs restantes           : {urls_restantes}")
print(f"   â€¢ @ restants               : {at_restants}")

if urls_restantes == 0 and at_restants == 0:
    print("   âœ… SUCCÃˆS TOTAL : Toutes les URLs et @ supprimÃ©s !")
else:
    print(f"   âš ï¸  ATTENTION : {urls_restantes} URLs et {at_restants} @ restants")

import re
texte = "Ø¹Ù†Ø¯ÙŠ Ø§Ø´ØªØ±Ø§Ùƒ 1ØŒ5 Ø¬ÙŠØºØ§ https://"
resultat = re.sub(r'https?://\S*|www\.\S+', '', texte).strip()
print(resultat)
# â†’ "Ø¹Ù†Ø¯ÙŠ Ø§Ø´ØªØ±Ø§Ùƒ 1ØŒ5 Ø¬ÙŠØºØ§"  âœ…


# ============================================================
# 8. RAPPORT FINAL
# ============================================================
temps_fin_global = time.time()
temps_total = temps_fin_global - temps_debut_global

rapport = f"""
{"="*70}
RAPPORT â€” SUPPRESSION URLS & @ â€” VERSION DISTRIBUÃ‰E (mapPartitions)
{"="*70}
Date             : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
Mode             : Spark 4.1.1 | mapPartitions | Workers â†’ MongoDB direct
RÃ©seau Docker    : projet_telecom_spark_network
URI Workers      : {MONGO_URI_WORKERS}

â±ï¸  TEMPS D'EXÃ‰CUTION:
   â€¢ Connexion Spark     : {temps_fin_spark - temps_debut_spark:.2f}s
   â€¢ Chargement donnÃ©es  : {temps_fin_chargement - temps_debut_chargement:.2f}s
   â€¢ Analyse             : {temps_fin_analyse - temps_debut_analyse:.2f}s
   â€¢ Nettoyage           : {temps_fin_nettoyage - temps_debut_nettoyage:.2f}s
   â€¢ Ã‰criture MongoDB    : {temps_fin_sauvegarde - temps_debut_sauvegarde:.2f}s
   â€¢ VÃ©rification        : {temps_fin_verif - temps_debut_verif:.2f}s
   â€¢ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   â€¢ TEMPS TOTAL         : {temps_total:.2f}s
   â€¢ Vitesse             : {total / temps_total:.2f} doc/s

ğŸ“Š RÃ‰SULTATS:
   â€¢ Total documents        : {total}
   â€¢ URLs supprimÃ©es        : {avec_urls_av - avec_urls_ap} / {avec_urls_av} ({taux_urls:.2f}%)
   â€¢ @ supprimÃ©s            : {avec_at_av - avec_at_ap} / {avec_at_av} ({taux_at:.2f}%)
   â€¢ URLs restantes         : {urls_restantes}
   â€¢ @ restants             : {at_restants}

ğŸ“ STOCKAGE:
   â€¢ Source      : {DB_NAME}.{COLLECTION_SOURCE}
   â€¢ Destination : {DB_NAME}.{COLLECTION_DEST}
   â€¢ InsÃ©rÃ©s     : {total_inseres}
   â€¢ Statut      : {"âœ… SUCCÃˆS" if (urls_restantes == 0 and at_restants == 0) else "âš ï¸ INCOMPLET"}

ğŸ—ï¸  ARCHITECTURE:
   MongoDB â”€â”€â–º Driver (prÃ©pare les plages skip/limit)
                  â†™                    â†˜
             Worker 1              Worker 2
          (lit + nettoie)       (lit + nettoie)
                  â†“                    â†“
             MongoDB               MongoDB
           (Ã©crit direct)        (Ã©crit direct)
"""

os.makedirs(os.path.dirname(RAPPORT_PATH), exist_ok=True)
with open(RAPPORT_PATH, "w", encoding="utf-8") as f:
    f.write(rapport)
print(f"\nâœ… Rapport sauvegardÃ© : {RAPPORT_PATH}")

# ============================================================
# RÃ‰SUMÃ‰ CONSOLE
# ============================================================
print("\n" + "="*70)
print("ğŸ“Š RÃ‰SUMÃ‰ FINAL")
print("="*70)
print(f"ğŸ“¥ Documents traitÃ©s  : {total}")
print(f"ğŸ“¤ Documents insÃ©rÃ©s  : {total_inseres}")
print(f"ğŸ—‘ï¸  URLs supprimÃ©es    : {avec_urls_av - avec_urls_ap} ({taux_urls:.1f}%)")
print(f"ğŸ—‘ï¸  @ supprimÃ©s        : {avec_at_av - avec_at_ap} ({taux_at:.1f}%)")
print(f"â±ï¸  Temps total        : {temps_total:.2f}s")
print(f"ğŸš€ Vitesse            : {total/temps_total:.0f} docs/s")
print(f"ğŸ“ Collection dest.   : {DB_NAME}.{COLLECTION_DEST}")
print("="*70)
print("ğŸ‰ PIPELINE DISTRIBUÃ‰ TERMINÃ‰ !")

# ============================================================
# FERMETURE PROPRE
# ============================================================
df_nettoye.unpersist()
spark.stop()
client_driver.close()
print("ğŸ”Œ Connexions fermÃ©es proprement")