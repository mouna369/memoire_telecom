# #!/usr/bin/env python3
# # -*- coding: utf-8 -*-

# # scripts/nettoyage/01_supprimer_urls_multinode.py - VERSION CORRIGÃ‰E

# from pyspark.sql import SparkSession
# from pyspark.sql.functions import col, udf, spark_partition_id
# from pyspark.sql.types import StringType, IntegerType
# import re
# from pymongo import MongoClient
# from datetime import datetime
# import os
# import socket
# import pandas as pd  # ğŸ‘ˆ IMPORT MANQUANT AJOUTÃ‰

# def supprimer_urls(texte):
#     """Supprime les URLs d'un texte - Version amÃ©liorÃ©e"""
#     if texte is None or not isinstance(texte, str):
#         return texte
    
#     # Patterns amÃ©liorÃ©s pour dÃ©tecter tous les types d'URLs
#     patterns = [
#         r'https?://\S+',           # URLs complÃ¨tes
#         r'www\.\S+',                # www.example.com
#         r'https?://(?:\s|$)',       # https:// seul suivi d'espace ou fin
#         r'https?://$',              # https:// en fin de chaÃ®ne
#         r'\bhttps?://\b',           # https:// comme mot isolÃ©
#         r'http://(?:\s|$)',         # http:// seul
#         r'http://$'                 # http:// en fin de chaÃ®ne
#     ]
    
#     texte_propre = texte
#     for pattern in patterns:
#         texte_propre = re.sub(pattern, '', texte_propre, flags=re.IGNORECASE)
    
#     # Supprimer les espaces multiples
#     texte_propre = re.sub(r'\s+', ' ', texte_propre).strip()
#     return texte_propre if texte_propre else None

# def detecter_urls(texte):
#     """DÃ©tecte si un texte contient des URLs"""
#     if texte is None or not isinstance(texte, str):
#         return 0
    
#     patterns = [
#         r'https?://',
#         r'www\.',
#         r'https?://(?:\s|$)',
#         r'https?://$'
#     ]
    
#     for pattern in patterns:
#         if re.search(pattern, texte, re.IGNORECASE):
#             return 1
#     return 0

# def compter_urls(texte):
#     """Compte le nombre d'URLs dans un texte"""
#     if texte is None or not isinstance(texte, str):
#         return 0
    
#     pattern = r'https?://\S+|www\.\S+|https?://(?:\s|$)|https?://$'
#     return len(re.findall(pattern, texte, re.IGNORECASE))

# def get_worker_name():
#     """Retourne le nom du worker"""
#     return socket.gethostname()

# print("="*70)
# print("ğŸ” Ã‰TAPE 1 : SUPPRESSION DES URLS - MODE MULTI-NODE")
# print("="*70)

# # 1. CONNEXION Ã€ MONGODB
# print("\nğŸ“‚ Connexion Ã  MongoDB...")
# try:
#     client = MongoClient('localhost', 27018)
#     db = client['telecom_algerie']
#     collection_source = db['commentaires_bruts']
#     total_docs = collection_source.count_documents({})
#     print(f"âœ… Connexion MongoDB rÃ©ussie")
#     print(f"ğŸ“Š Collection source: {total_docs} documents")
# except Exception as e:
#     print(f"âŒ Erreur de connexion MongoDB: {e}")
#     exit(1)

# # 2. CONNEXION AU CLUSTER SPARK
# print("\nâš¡ Connexion au cluster Spark multi-node...")

# spark = SparkSession.builder \
#     .appName("Suppression_URLs_MultiNode") \
#     .master("spark://localhost:7077") \
#     .config("spark.executor.memory", "2g") \
#     .config("spark.executor.cores", "2") \
#     .config("spark.sql.execution.arrow.pyspark.enabled", "false") \
#     .getOrCreate()

# print("âœ… Cluster Spark multi-node connectÃ©")

# # 3. CHARGER LES DONNÃ‰ES AVEC PYMONGO
# print("\nğŸ“¥ Chargement des donnÃ©es avec PyMongo...")

# # Charger tous les documents
# print("   RÃ©cupÃ©ration des documents...")
# data = list(collection_source.find({}))
# print(f"   ğŸ“Š {len(data)} documents chargÃ©s")

# # Convertir les ObjectId en string
# print("   ğŸ”„ Conversion des ObjectId...")
# for doc in data:
#     doc['_id'] = str(doc['_id'])

# # CrÃ©er DataFrame Spark
# print("   ğŸ“Š CrÃ©ation du DataFrame Spark...")
# df_spark = spark.createDataFrame(data)
# total_lignes = df_spark.count()
# print(f"âœ… {total_lignes} documents chargÃ©s dans Spark")

# # 4. IDENTIFIER LES WORKERS
# print("\nğŸ” RÃ‰PARTITION SUR LES WORKERS:")

# worker_udf = udf(get_worker_name, StringType())

# df_with_workers = df_spark \
#     .withColumn("partition_id", spark_partition_id()) \
#     .withColumn("worker_name", worker_udf())

# print("   Distribution des donnÃ©es:")
# df_with_workers.groupBy("worker_name", "partition_id").count().show()

# # 5. ENREGISTRER LES UDF
# print("\nğŸ”„ Enregistrement des fonctions...")
# supprimer_urls_udf = udf(supprimer_urls, StringType())
# detecter_urls_udf = udf(detecter_urls, IntegerType())
# compter_urls_udf = udf(compter_urls, IntegerType())

# # 6. ANALYSE AVANT NETTOYAGE
# print("\nğŸ” ANALYSE : Recherche des URLs...")

# df_analyse = df_with_workers \
#     .withColumn("urls_avant", detecter_urls_udf(col("Commentaire_Client"))) \
#     .withColumn("nb_urls_avant", compter_urls_udf(col("Commentaire_Client")))

# total = df_analyse.count()
# avec_urls_avant = df_analyse.filter(col("urls_avant") == 1).count()
# total_urls = df_analyse.agg({"nb_urls_avant": "sum"}).collect()[0][0] or 0

# print(f"\nğŸ“Š STATISTIQUES AVANT NETTOYAGE:")
# print(f"   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
# print(f"   â”‚ Total documents        : {total:<15} â”‚")
# print(f"   â”‚ Documents avec URLs    : {avec_urls_avant:<15} â”‚")
# print(f"   â”‚ URLs dÃ©tectÃ©es         : {total_urls:<15} â”‚")
# print(f"   â”‚ Pourcentage            : {(avec_urls_avant/total*100):<15.2f}% â”‚")
# print(f"   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")

# # 7. NETTOYAGE
# print("\nğŸ§¹ SUPPRESSION DES URLS EN COURS...")

# df_nettoye = df_analyse \
#     .withColumn("Commentaire_Client_propre", supprimer_urls_udf(col("Commentaire_Client"))) \
#     .withColumn("commentaire_moderateur_propre", supprimer_urls_udf(col("commentaire_moderateur"))) \
#     .withColumn("urls_apres", detecter_urls_udf(col("Commentaire_Client_propre")))

# # 8. STATISTIQUES APRÃˆS NETTOYAGE
# avec_urls_apres = df_nettoye.filter(col("urls_apres") == 1).count()
# supprimees = avec_urls_avant - avec_urls_apres
# taux = (supprimees / avec_urls_avant * 100) if avec_urls_avant > 0 else 0

# print(f"\nğŸ“Š STATISTIQUES APRÃˆS NETTOYAGE:")
# print(f"   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
# print(f"   â”‚ Documents avec URLs avant : {avec_urls_avant:<15} â”‚")
# print(f"   â”‚ Documents avec URLs aprÃ¨s : {avec_urls_apres:<15} â”‚")
# print(f"   â”‚ URLs supprimÃ©es           : {supprimees:<15} â”‚")
# print(f"   â”‚ Taux de succÃ¨s            : {taux:<15.2f}% â”‚")
# print(f"   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")

# # 9. STATISTIQUES PAR WORKER
# print("\nğŸ“Š PERFORMANCE PAR WORKER:")
# worker_perf = df_nettoye.groupBy("worker_name").agg(
#     {"urls_avant": "sum", "urls_apres": "sum", "Commentaire_Client": "count"}
# ).withColumnRenamed("sum(urls_avant)", "urls_trouvees") \
#  .withColumnRenamed("sum(urls_apres)", "urls_restantes") \
#  .withColumnRenamed("count(Commentaire_Client)", "documents")

# worker_perf.show()

# # 10. PRÃ‰PARATION POUR MONGODB
# print("\nğŸ’¾ PRÃ‰PARATION POUR SAUVEGARDE...")

# df_final = df_nettoye.select(
#     "_id",
#     col("Commentaire_Client_propre").alias("Commentaire_Client"),
#     col("commentaire_moderateur_propre").alias("commentaire_moderateur"),
#     "date",
#     "source",
#     "moderateur",
#     "metadata",
#     "statut"
# )

# # 11. SAUVEGARDE DANS MONGODB
# print("\nğŸ“ SAUVEGARDE DANS MONGODB...")

# # Convertir en Pandas par lots pour Ã©viter les problÃ¨mes de mÃ©moire
# print("   ğŸ”„ Conversion en Pandas...")
# df_pandas = df_final.toPandas()
# print(f"   âœ… {len(df_pandas)} lignes converties")

# # Collection destination
# collection_dest = db['commentaires_sans_urls_multinode']
# collection_dest.delete_many({})
# print("   ğŸ§¹ Collection destination vidÃ©e")

# # InsÃ©rer par lots
# print("   ğŸ“¥ Insertion par lots...")
# batch_size = 500
# total_batches = (len(df_pandas) + batch_size - 1) // batch_size

# for i in range(0, len(df_pandas), batch_size):
#     batch_num = i//batch_size + 1
#     batch = df_pandas.iloc[i:i+batch_size].to_dict('records')
    
#     # Convertir les NaN en None pour MongoDB
#     for doc in batch:
#         for key, value in doc.items():
#             if pd.isna(value):  # ğŸ‘ˆ MAINTENANT PANDAS EST IMPORTÃ‰
#                 doc[key] = None
    
#     collection_dest.insert_many(batch)
#     print(f"   âœ“ Lot {batch_num}/{total_batches}: {len(batch)} documents")

# print(f"\nâœ… {len(df_pandas)} documents sauvegardÃ©s dans 'commentaires_sans_urls_multinode'")

# # 12. VÃ‰RIFICATION FINALE
# print("\nğŸ” VÃ‰RIFICATION FINALE...")

# # VÃ©rifier avec diffÃ©rents patterns
# patterns_verif = [
#     r'https?://\S+',
#     r'www\.\S+',
#     r'https?://(?:\s|$)',
#     r'https?://$'
# ]

# print("\n   VÃ©rification pattern par pattern:")
# for pattern in patterns_verif:
#     count = collection_dest.count_documents({
#         "Commentaire_Client": {"$regex": pattern, "$options": "i"}
#     })
#     print(f"   â€¢ {pattern[:20]}...: {count} documents")

# # VÃ©rification globale
# urls_restantes = collection_dest.count_documents({
#     "Commentaire_Client": {"$regex": "https?://|www\.", "$options": "i"}
# })

# print(f"\nğŸ“Š RÃ‰SULTAT FINAL:")
# print(f"   â€¢ Documents avec URLs restantes: {urls_restantes}")
# if urls_restantes == 0:
#     print("   âœ… SUCCÃˆS : Toutes les URLs ont Ã©tÃ© supprimÃ©es !")
# else:
#     print(f"   âš ï¸ ATTENTION : {urls_restantes} URLs restantes")

# # 13. RAPPORT
# print("\nğŸ“„ CRÃ‰ATION DU RAPPORT...")

# rapport = f"""
# {"="*70}
# RAPPORT DE SUPPRESSION DES URLS - MODE MULTI-NODE
# {"="*70}

# Date d'exÃ©cution : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
# Mode : Multi-node Spark Cluster

# ğŸ“Š STATISTIQUES GLOBALES:
#    â€¢ Total documents traitÃ©s    : {total}
#    â€¢ Documents avec URLs (avant): {avec_urls_avant}
#    â€¢ URLs dÃ©tectÃ©es (avant)     : {total_urls}
#    â€¢ Documents avec URLs (aprÃ¨s): {avec_urls_apres}
#    â€¢ URLs supprimÃ©es            : {supprimees}
#    â€¢ Taux de succÃ¨s             : {taux:.2f}%

# ğŸ“ STOCKAGE:
#    â€¢ Collection source      : telecom_algerie.commentaires_bruts
#    â€¢ Collection destination : telecom_algerie.commentaires_sans_urls_multinode
#    â€¢ Documents sauvegardÃ©s  : {len(df_pandas)}

# ğŸ” VÃ‰RIFICATION FINALE:
#    â€¢ URLs restantes dÃ©tectÃ©es : {urls_restantes}
#    â€¢ Statut : {"âœ… SUCCÃˆS" if urls_restantes == 0 else "âš ï¸ Ã‰CHEC"}

# âš¡ DISTRIBUTION:
#    â€¢ Workers utilisÃ©s : {df_with_workers.select("worker_name").distinct().count()}
#    â€¢ Voir dÃ©tails dans les logs pour la rÃ©partition
# """

# # Sauvegarder le rapport
# os.makedirs("donnees/resultats", exist_ok=True)
# rapport_path = "donnees/resultats/rapport_urls_multinode.txt"
# with open(rapport_path, "w", encoding="utf-8") as f:
#     f.write(rapport)
# print(f"âœ… Rapport sauvegardÃ©: {rapport_path}")

# # 14. RÃ‰SUMÃ‰ FINAL
# print("\n" + "="*70)
# print("ğŸ“Š RÃ‰SUMÃ‰ FINAL - MODE MULTI-NODE")
# print("="*70)
# print(f"ğŸ“¥ Documents traitÃ©s    : {total}")
# print(f"ğŸ”— URLs dÃ©tectÃ©es       : {total_urls}")
# print(f"ğŸ“ Documents avec URLs  : {avec_urls_avant}")
# print(f"âœ… URLs supprimÃ©es      : {supprimees}")
# print(f"ğŸ“ˆ Taux de succÃ¨s       : {taux:.2f}%")
# print(f"\nğŸ“ Collection MongoDB:")
# print(f"   â€¢ telecom_algerie.commentaires_sans_urls_multinode")
# print("="*70)

# print("\nğŸ‰ SUPPRESSION DES URLS TERMINÃ‰E EN MODE MULTI-NODE !")

# # Fermer les connexions
# spark.stop()
# client.close()
# print("\nğŸ”Œ Connexions fermÃ©es")




# #!/usr/bin/env python3
# # -*- coding: utf-8 -*-

# # scripts/nettoyage/01_supprimer_urls_multinode.py - VERSION AVEC MESURE DE TEMPS

# from pyspark.sql import SparkSession
# from pyspark.sql.functions import col, udf, spark_partition_id
# from pyspark.sql.types import StringType, IntegerType
# import re
# from pymongo import MongoClient
# from datetime import datetime
# import os
# import socket
# import pandas as pd
# import time  # ğŸ‘ˆ POUR MESURER LE TEMPS

# def supprimer_urls(texte):
#     """Supprime les URLs d'un texte - Version amÃ©liorÃ©e"""
#     if texte is None or not isinstance(texte, str):
#         return texte
    
#     # Patterns amÃ©liorÃ©s pour dÃ©tecter tous les types d'URLs
#     patterns = [
#         r'https?://\S+',           # URLs complÃ¨tes
#         r'www\.\S+',                # www.example.com
#         r'https?://(?:\s|$)',       # https:// seul suivi d'espace ou fin
#         r'https?://$',              # https:// en fin de chaÃ®ne
#         r'\bhttps?://\b',           # https:// comme mot isolÃ©
#         r'http://(?:\s|$)',         # http:// seul
#         r'http://$'                 # http:// en fin de chaÃ®ne
#     ]
    
#     texte_propre = texte
#     for pattern in patterns:
#         texte_propre = re.sub(pattern, '', texte_propre, flags=re.IGNORECASE)
    
#     # Supprimer les espaces multiples
#     texte_propre = re.sub(r'\s+', ' ', texte_propre).strip()
#     return texte_propre if texte_propre else None

# def detecter_urls(texte):
#     """DÃ©tecte si un texte contient des URLs"""
#     if texte is None or not isinstance(texte, str):
#         return 0
    
#     patterns = [
#         r'https?://',
#         r'www\.',
#         r'https?://(?:\s|$)',
#         r'https?://$'
#     ]
    
#     for pattern in patterns:
#         if re.search(pattern, texte, re.IGNORECASE):
#             return 1
#     return 0

# def compter_urls(texte):
#     """Compte le nombre d'URLs dans un texte"""
#     if texte is None or not isinstance(texte, str):
#         return 0
    
#     pattern = r'https?://\S+|www\.\S+|https?://(?:\s|$)|https?://$'
#     return len(re.findall(pattern, texte, re.IGNORECASE))

# def get_worker_name():
#     """Retourne le nom du worker"""
#     return socket.gethostname()

# # ğŸ“Š DÃ‰BUT DU CHRONOMÃˆTRAGE GLOBAL
# temps_debut_global = time.time()

# print("="*70)
# print("ğŸ” Ã‰TAPE 1 : SUPPRESSION DES URLS - MODE MULTI-NODE")
# print("="*70)

# # 1. CONNEXION Ã€ MONGODB
# print("\nğŸ“‚ Connexion Ã  MongoDB...")
# try:
#     client = MongoClient('localhost', 27018)
#     db = client['telecom_algerie']
#     collection_source = db['commentaires_bruts']
#     total_docs = collection_source.count_documents({})
#     print(f"âœ… Connexion MongoDB rÃ©ussie")
#     print(f"ğŸ“Š Collection source: {total_docs} documents")
# except Exception as e:
#     print(f"âŒ Erreur de connexion MongoDB: {e}")
#     exit(1)

# # 2. CONNEXION AU CLUSTER SPARK
# print("\nâš¡ Connexion au cluster Spark multi-node...")
# temps_debut_spark = time.time()

# # spark = SparkSession.builder \
# #     .appName("Suppression_URLs_MultiNode") \
# #     .master("spark://localhost:7077") \
# #     .config("spark.executor.memory", "2g") \
# #     .config("spark.executor.cores", "2") \
# #     .config("spark.sql.execution.arrow.pyspark.enabled", "false") \
# #     .getOrCreate()

# spark = SparkSession.builder \
#     .appName("Suppression_URLs_MultiNode") \
#     .master("spark://spark-master:7077") \
#     .config("spark.executor.memory", "2g") \
#     .config("spark.executor.cores", "12") \
#     .getOrCreate()

# temps_fin_spark = time.time()
# print(f"âœ… Cluster Spark multi-node connectÃ© en {temps_fin_spark - temps_debut_spark:.2f} secondes")

# # 3. CHARGER LES DONNÃ‰ES AVEC PYMONGO
# print("\nğŸ“¥ Chargement des donnÃ©es avec PyMongo...")
# temps_debut_chargement = time.time()

# # Charger tous les documents
# print("   RÃ©cupÃ©ration des documents...")
# data = list(collection_source.find({}))
# print(f"   ğŸ“Š {len(data)} documents chargÃ©s")

# # Convertir les ObjectId en string
# print("   ğŸ”„ Conversion des ObjectId...")
# for doc in data:
#     doc['_id'] = str(doc['_id'])

# # CrÃ©er DataFrame Spark
# print("   ğŸ“Š CrÃ©ation du DataFrame Spark...")
# df_spark = spark.createDataFrame(data)
# total_lignes = df_spark.count()

# temps_fin_chargement = time.time()
# print(f"âœ… {total_lignes} documents chargÃ©s dans Spark en {temps_fin_chargement - temps_debut_chargement:.2f} secondes")

# # 4. IDENTIFIER LES WORKERS
# print("\nğŸ” RÃ‰PARTITION SUR LES WORKERS:")

# worker_udf = udf(get_worker_name, StringType())

# df_with_workers = df_spark \
#     .withColumn("partition_id", spark_partition_id()) \
#     .withColumn("worker_name", worker_udf())

# print("   Distribution des donnÃ©es:")
# df_with_workers.groupBy("worker_name", "partition_id").count().show()

# # 5. ENREGISTRER LES UDF
# print("\nğŸ”„ Enregistrement des fonctions...")
# supprimer_urls_udf = udf(supprimer_urls, StringType())
# detecter_urls_udf = udf(detecter_urls, IntegerType())
# compter_urls_udf = udf(compter_urls, IntegerType())

# # 6. ANALYSE AVANT NETTOYAGE
# print("\nğŸ” ANALYSE : Recherche des URLs...")
# temps_debut_analyse = time.time()

# df_analyse = df_with_workers \
#     .withColumn("urls_avant", detecter_urls_udf(col("Commentaire_Client"))) \
#     .withColumn("nb_urls_avant", compter_urls_udf(col("Commentaire_Client")))

# total = df_analyse.count()
# avec_urls_avant = df_analyse.filter(col("urls_avant") == 1).count()
# total_urls = df_analyse.agg({"nb_urls_avant": "sum"}).collect()[0][0] or 0

# temps_fin_analyse = time.time()
# print(f"\nğŸ“Š STATISTIQUES AVANT NETTOYAGE (analyse en {temps_fin_analyse - temps_debut_analyse:.2f}s):")
# print(f"   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
# print(f"   â”‚ Total documents        : {total:<15} â”‚")
# print(f"   â”‚ Documents avec URLs    : {avec_urls_avant:<15} â”‚")
# print(f"   â”‚ URLs dÃ©tectÃ©es         : {total_urls:<15} â”‚")
# print(f"   â”‚ Pourcentage            : {(avec_urls_avant/total*100):<15.2f}% â”‚")
# print(f"   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")

# # 7. NETTOYAGE
# print("\nğŸ§¹ SUPPRESSION DES URLS EN COURS...")
# temps_debut_nettoyage = time.time()

# df_nettoye = df_analyse \
#     .withColumn("Commentaire_Client_propre", supprimer_urls_udf(col("Commentaire_Client"))) \
#     .withColumn("commentaire_moderateur_propre", supprimer_urls_udf(col("commentaire_moderateur"))) \
#     .withColumn("urls_apres", detecter_urls_udf(col("Commentaire_Client_propre")))

# # Forcer l'exÃ©cution des transformations
# df_nettoye.cache().count()

# temps_fin_nettoyage = time.time()
# print(f"âœ… Nettoyage terminÃ© en {temps_fin_nettoyage - temps_debut_nettoyage:.2f} secondes")

# # 8. STATISTIQUES APRÃˆS NETTOYAGE
# avec_urls_apres = df_nettoye.filter(col("urls_apres") == 1).count()
# supprimees = avec_urls_avant - avec_urls_apres
# taux = (supprimees / avec_urls_avant * 100) if avec_urls_avant > 0 else 0

# print(f"\nğŸ“Š STATISTIQUES APRÃˆS NETTOYAGE:")
# print(f"   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
# print(f"   â”‚ Documents avec URLs avant : {avec_urls_avant:<15} â”‚")
# print(f"   â”‚ Documents avec URLs aprÃ¨s : {avec_urls_apres:<15} â”‚")
# print(f"   â”‚ URLs supprimÃ©es           : {supprimees:<15} â”‚")
# print(f"   â”‚ Taux de succÃ¨s            : {taux:<15.2f}% â”‚")
# print(f"   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")

# # 9. STATISTIQUES PAR WORKER
# print("\nğŸ“Š PERFORMANCE PAR WORKER:")
# worker_perf = df_nettoye.groupBy("worker_name").agg(
#     {"urls_avant": "sum", "urls_apres": "sum", "Commentaire_Client": "count"}
# ).withColumnRenamed("sum(urls_avant)", "urls_trouvees") \
#  .withColumnRenamed("sum(urls_apres)", "urls_restantes") \
#  .withColumnRenamed("count(Commentaire_Client)", "documents")

# worker_perf.show()

# # 10. PRÃ‰PARATION POUR MONGODB
# print("\nğŸ’¾ PRÃ‰PARATION POUR SAUVEGARDE...")
# temps_debut_preparation = time.time()

# df_final = df_nettoye.select(
#     "_id",
#     col("Commentaire_Client_propre").alias("Commentaire_Client"),
#     col("commentaire_moderateur_propre").alias("commentaire_moderateur"),
#     "date",
#     "source",
#     "moderateur",
#     "metadata",
#     "statut"
# )

# temps_fin_preparation = time.time()
# print(f"âœ… PrÃ©paration terminÃ©e en {temps_fin_preparation - temps_debut_preparation:.2f} secondes")

# # 11. SAUVEGARDE DANS MONGODB
# print("\nğŸ“ SAUVEGARDE DANS MONGODB...")
# temps_debut_sauvegarde = time.time()

# # Convertir en Pandas
# print("   ğŸ”„ Conversion en Pandas...")
# df_pandas = df_final.toPandas()
# print(f"   âœ… {len(df_pandas)} lignes converties")

# # Collection destination
# collection_dest = db['commentaires_sans_urls_multinode2']
# collection_dest.delete_many({})
# print("   ğŸ§¹ Collection destination vidÃ©e")

# # InsÃ©rer par lots
# print("   ğŸ“¥ Insertion par lots...")
# batch_size = 500
# total_batches = (len(df_pandas) + batch_size - 1) // batch_size

# for i in range(0, len(df_pandas), batch_size):
#     batch_num = i//batch_size + 1
#     batch = df_pandas.iloc[i:i+batch_size].to_dict('records')
    
#     # Convertir les NaN en None pour MongoDB
#     for doc in batch:
#         for key, value in doc.items():
#             if pd.isna(value):
#                 doc[key] = None
    
#     collection_dest.insert_many(batch)
#     print(f"   âœ“ Lot {batch_num}/{total_batches}: {len(batch)} documents")

# temps_fin_sauvegarde = time.time()
# print(f"\nâœ… {len(df_pandas)} documents sauvegardÃ©s dans 'commentaires_sans_urls_multinode2'")
# print(f"   â±ï¸  Temps de sauvegarde: {temps_fin_sauvegarde - temps_debut_sauvegarde:.2f} secondes")

# # 12. VÃ‰RIFICATION FINALE
# print("\nğŸ” VÃ‰RIFICATION FINALE...")
# temps_debut_verification = time.time()

# # VÃ©rifier avec diffÃ©rents patterns
# patterns_verif = [
#     r'https?://\S+',
#     r'www\.\S+',
#     r'https?://(?:\s|$)',
#     r'https?://$'
# ]

# print("\n   VÃ©rification pattern par pattern:")
# for pattern in patterns_verif:
#     count = collection_dest.count_documents({
#         "Commentaire_Client": {"$regex": pattern, "$options": "i"}
#     })
#     print(f"   â€¢ {pattern[:20]}...: {count} documents")

# # VÃ©rification globale
# urls_restantes = collection_dest.count_documents({
#     "Commentaire_Client": {"$regex": "https?://|www\.", "$options": "i"}
# })

# temps_fin_verification = time.time()
# print(f"\nğŸ“Š RÃ‰SULTAT FINAL (vÃ©rification en {temps_fin_verification - temps_debut_verification:.2f}s):")
# print(f"   â€¢ Documents avec URLs restantes: {urls_restantes}")
# if urls_restantes == 0:
#     print("   âœ… SUCCÃˆS : Toutes les URLs ont Ã©tÃ© supprimÃ©es !")
# else:
#     print(f"   âš ï¸ ATTENTION : {urls_restantes} URLs restantes")

# # ğŸ FIN DU CHRONOMÃˆTRAGE GLOBAL
# temps_fin_global = time.time()
# temps_total = temps_fin_global - temps_debut_global

# # 13. RAPPORT AVEC TEMPS D'EXÃ‰CUTION
# print("\nğŸ“„ CRÃ‰ATION DU RAPPORT...")

# rapport = f"""
# {"="*70}
# RAPPORT DE SUPPRESSION DES URLS - MODE MULTI-NODE
# {"="*70}

# Date d'exÃ©cution : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
# Mode : Multi-node Spark Cluster

# â±ï¸  TEMPS D'EXÃ‰CUTION:
#    â€¢ Connexion Spark        : {temps_fin_spark - temps_debut_spark:.2f}s
#    â€¢ Chargement donnÃ©es     : {temps_fin_chargement - temps_debut_chargement:.2f}s
#    â€¢ Analyse des URLs       : {temps_fin_analyse - temps_debut_analyse:.2f}s
#    â€¢ Nettoyage URLs         : {temps_fin_nettoyage - temps_debut_nettoyage:.2f}s
#    â€¢ PrÃ©paration DataFrame  : {temps_fin_preparation - temps_debut_preparation:.2f}s
#    â€¢ Sauvegarde MongoDB     : {temps_fin_sauvegarde - temps_debut_sauvegarde:.2f}s
#    â€¢ VÃ©rification           : {temps_fin_verification - temps_debut_verification:.2f}s
#    â€¢ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#    â€¢ TEMPS TOTAL            : {temps_total:.2f}s
#    â€¢ Documents par seconde  : {total / temps_total:.2f} doc/s

# ğŸ“Š STATISTIQUES GLOBALES:
#    â€¢ Total documents traitÃ©s    : {total}
#    â€¢ Documents avec URLs (avant): {avec_urls_avant}
#    â€¢ URLs dÃ©tectÃ©es (avant)     : {total_urls}
#    â€¢ Documents avec URLs (aprÃ¨s): {avec_urls_apres}
#    â€¢ URLs supprimÃ©es            : {supprimees}
#    â€¢ Taux de succÃ¨s             : {taux:.2f}%

# ğŸ“ STOCKAGE:
#    â€¢ Collection source      : telecom_algerie.commentaires_bruts
#    â€¢ Collection destination : telecom_algerie.commentaires_sans_urls_multinode2
#    â€¢ Documents sauvegardÃ©s  : {len(df_pandas)}

# ğŸ” VÃ‰RIFICATION FINALE:
#    â€¢ URLs restantes dÃ©tectÃ©es : {urls_restantes}
#    â€¢ Statut : {"âœ… SUCCÃˆS" if urls_restantes == 0 else "âš ï¸ Ã‰CHEC"}

# âš¡ DISTRIBUTION:
#    â€¢ Workers utilisÃ©s : {df_with_workers.select("worker_name").distinct().count()}
# """

# # Sauvegarder le rapport
# os.makedirs("donnees/resultats", exist_ok=True)
# rapport_path = "donnees/resultats/rapport_urls_multinode2.txt"
# with open(rapport_path, "w", encoding="utf-8") as f:
#     f.write(rapport)
# print(f"âœ… Rapport sauvegardÃ©: {rapport_path}")

# # 14. RÃ‰SUMÃ‰ FINAL AVEC TEMPS
# print("\n" + "="*70)
# print("ğŸ“Š RÃ‰SUMÃ‰ FINAL - MODE MULTI-NODE")
# print("="*70)
# print(f"ğŸ“¥ Documents traitÃ©s    : {total}")
# print(f"ğŸ”— URLs dÃ©tectÃ©es       : {total_urls}")
# print(f"ğŸ“ Documents avec URLs  : {avec_urls_avant}")
# print(f"âœ… URLs supprimÃ©es      : {supprimees}")
# print(f"ğŸ“ˆ Taux de succÃ¨s       : {taux:.2f}%")
# print(f"\nâ±ï¸  TEMPS D'EXÃ‰CUTION:")
# print(f"   â€¢ Chargement : {temps_fin_chargement - temps_debut_chargement:.2f}s")
# print(f"   â€¢ Nettoyage  : {temps_fin_nettoyage - temps_debut_nettoyage:.2f}s")
# print(f"   â€¢ Sauvegarde : {temps_fin_sauvegarde - temps_debut_sauvegarde:.2f}s")
# print(f"   â€¢ TOTAL      : {temps_total:.2f}s")
# print(f"   â€¢ Vitesse    : {total / temps_total:.2f} docs/s")
# print(f"\nğŸ“ Collection MongoDB:")
# print(f"   â€¢ telecom_algerie.commentaires_sans_urls_multinode2")
# print("="*70)

# print("\nğŸ‰ SUPPRESSION DES URLS TERMINÃ‰E EN MODE MULTI-NODE !")

# # Fermer les connexions
# spark.stop()
# client.close()
# print("\nğŸ”Œ Connexions fermÃ©es")

#!/usr/bin/env python3
# -*- coding: utf-8 -*-

# scripts/nettoyage/01_supprimer_urls_multinode.py - VERSION AVEC MESURE DE TEMPS

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, udf, spark_partition_id
from pyspark.sql.types import StringType, IntegerType
import re
from pymongo import MongoClient
from datetime import datetime
import os
import socket
import pandas as pd
import time  # ğŸ‘ˆ POUR MESURER LE TEMPS

def supprimer_urls(texte):
    """Supprime les URLs d'un texte - Version amÃ©liorÃ©e"""
    if texte is None or not isinstance(texte, str):
        return texte
    
    # Patterns amÃ©liorÃ©s pour dÃ©tecter tous les types d'URLs
    patterns = [
        r'https?://\S+',           # URLs complÃ¨tes
        r'www\.\S+',                # www.example.com
        r'https?://(?:\s|$)',       # https:// seul suivi d'espace ou fin
        r'https?://$',              # https:// en fin de chaÃ®ne
        r'\bhttps?://\b',           # https:// comme mot isolÃ©
        r'http://(?:\s|$)',         # http:// seul
        r'http://$'                 # http:// en fin de chaÃ®ne
    ]
    
    texte_propre = texte
    for pattern in patterns:
        texte_propre = re.sub(pattern, '', texte_propre, flags=re.IGNORECASE)
    
    # Supprimer les espaces multiples
    texte_propre = re.sub(r'\s+', ' ', texte_propre).strip()
    return texte_propre if texte_propre else None

def supprimer_at(texte):
    """Supprime les caractÃ¨res @ d'un texte"""
    if texte is None or not isinstance(texte, str):
        return texte
    
    # Supprimer tous les @
    texte_propre = re.sub(r'@', '', texte)
    
    # Supprimer les espaces multiples
    texte_propre = re.sub(r'\s+', ' ', texte_propre).strip()
    return texte_propre if texte_propre else None

def detecter_urls(texte):
    """DÃ©tecte si un texte contient des URLs"""
    if texte is None or not isinstance(texte, str):
        return 0
    
    patterns = [
        r'https?://',
        r'www\.',
        r'https?://(?:\s|$)',
        r'https?://$'
    ]
    
    for pattern in patterns:
        if re.search(pattern, texte, re.IGNORECASE):
            return 1
    return 0

def detecter_at(texte):
    """DÃ©tecte si un texte contient des @"""
    if texte is None or not isinstance(texte, str):
        return 0
    
    return 1 if re.search(r'@', texte) else 0

def compter_at(texte):
    """Compte le nombre de @ dans un texte"""
    if texte is None or not isinstance(texte, str):
        return 0
    
    return len(re.findall(r'@', texte))

def compter_urls(texte):
    """Compte le nombre d'URLs dans un texte"""
    if texte is None or not isinstance(texte, str):
        return 0
    
    pattern = r'https?://\S+|www\.\S+|https?://(?:\s|$)|https?://$'
    return len(re.findall(pattern, texte, re.IGNORECASE))

def get_worker_name():
    """Retourne le nom du worker"""
    return socket.gethostname()

# ğŸ“Š DÃ‰BUT DU CHRONOMÃˆTRAGE GLOBAL
temps_debut_global = time.time()

print("="*70)
print("ğŸ” Ã‰TAPE 1 : SUPPRESSION DES URLS ET DES @ - MODE MULTI-NODE")
print("="*70)

# 1. CONNEXION Ã€ MONGODB
print("\nğŸ“‚ Connexion Ã  MongoDB...")
try:
    client = MongoClient('localhost', 27018)
    db = client['telecom_algerie']
    collection_source = db['commentaires_bruts']
    total_docs = collection_source.count_documents({})
    print(f"âœ… Connexion MongoDB rÃ©ussie")
    print(f"ğŸ“Š Collection source: {total_docs} documents")
except Exception as e:
    print(f"âŒ Erreur de connexion MongoDB: {e}")
    exit(1)

# 2. CONNEXION AU CLUSTER SPARK
print("\nâš¡ Connexion au cluster Spark multi-node...")
temps_debut_spark = time.time()

spark = SparkSession.builder \
    .appName("Suppression_URLs_MultiNode") \
    .master("spark://spark-master:7077") \
    .config("spark.executor.memory", "2g") \
    .config("spark.executor.cores", "12") \
    .getOrCreate()

temps_fin_spark = time.time()
print(f"âœ… Cluster Spark multi-node connectÃ© en {temps_fin_spark - temps_debut_spark:.2f} secondes")

# 3. CHARGER LES DONNÃ‰ES AVEC PYMONGO
print("\nğŸ“¥ Chargement des donnÃ©es avec PyMongo...")
temps_debut_chargement = time.time()

# Charger tous les documents
print("   RÃ©cupÃ©ration des documents...")
data = list(collection_source.find({}))
print(f"   ğŸ“Š {len(data)} documents chargÃ©s")

# Convertir les ObjectId en string
print("   ğŸ”„ Conversion des ObjectId...")
for doc in data:
    doc['_id'] = str(doc['_id'])

# CrÃ©er DataFrame Spark
print("   ğŸ“Š CrÃ©ation du DataFrame Spark...")
df_spark = spark.createDataFrame(data)
total_lignes = df_spark.count()

temps_fin_chargement = time.time()
print(f"âœ… {total_lignes} documents chargÃ©s dans Spark en {temps_fin_chargement - temps_debut_chargement:.2f} secondes")

# 4. IDENTIFIER LES WORKERS
print("\nğŸ” RÃ‰PARTITION SUR LES WORKERS:")

worker_udf = udf(get_worker_name, StringType())

df_with_workers = df_spark \
    .withColumn("partition_id", spark_partition_id()) \
    .withColumn("worker_name", worker_udf())

print("   Distribution des donnÃ©es:")
df_with_workers.groupBy("worker_name", "partition_id").count().show()

# 5. ENREGISTRER LES UDF
print("\nğŸ”„ Enregistrement des fonctions...")
supprimer_urls_udf = udf(supprimer_urls, StringType())
supprimer_at_udf = udf(supprimer_at, StringType())
detecter_urls_udf = udf(detecter_urls, IntegerType())
detecter_at_udf = udf(detecter_at, IntegerType())
compter_urls_udf = udf(compter_urls, IntegerType())
compter_at_udf = udf(compter_at, IntegerType())

# 6. ANALYSE AVANT NETTOYAGE
print("\nğŸ” ANALYSE : Recherche des URLs et des @...")
temps_debut_analyse = time.time()

df_analyse = df_with_workers \
    .withColumn("urls_avant", detecter_urls_udf(col("Commentaire_Client"))) \
    .withColumn("nb_urls_avant", compter_urls_udf(col("Commentaire_Client"))) \
    .withColumn("at_avant", detecter_at_udf(col("Commentaire_Client"))) \
    .withColumn("nb_at_avant", compter_at_udf(col("Commentaire_Client")))

total = df_analyse.count()
avec_urls_avant = df_analyse.filter(col("urls_avant") == 1).count()
total_urls = df_analyse.agg({"nb_urls_avant": "sum"}).collect()[0][0] or 0
avec_at_avant = df_analyse.filter(col("at_avant") == 1).count()
total_at = df_analyse.agg({"nb_at_avant": "sum"}).collect()[0][0] or 0

temps_fin_analyse = time.time()
print(f"\nğŸ“Š STATISTIQUES AVANT NETTOYAGE (analyse en {temps_fin_analyse - temps_debut_analyse:.2f}s):")
print(f"   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
print(f"   â”‚ Total documents        : {total:<15} â”‚")
print(f"   â”‚ Documents avec URLs    : {avec_urls_avant:<15} â”‚")
print(f"   â”‚ URLs dÃ©tectÃ©es         : {total_urls:<15} â”‚")
print(f"   â”‚ Documents avec @       : {avec_at_avant:<15} â”‚")
print(f"   â”‚ @ dÃ©tectÃ©s             : {total_at:<15} â”‚")
print(f"   â”‚ Pourcentage URLs       : {(avec_urls_avant/total*100):<15.2f}% â”‚")
print(f"   â”‚ Pourcentage @          : {(avec_at_avant/total*100):<15.2f}% â”‚")
print(f"   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")

# 7. NETTOYAGE
print("\nğŸ§¹ SUPPRESSION DES URLS ET DES @ EN COURS...")
temps_debut_nettoyage = time.time()

# Appliquer d'abord la suppression des URLs, puis la suppression des @
df_nettoye = df_analyse \
    .withColumn("Commentaire_Client_sans_urls", supprimer_urls_udf(col("Commentaire_Client"))) \
    .withColumn("commentaire_moderateur_sans_urls", supprimer_urls_udf(col("commentaire_moderateur"))) \
    .withColumn("Commentaire_Client_propre", supprimer_at_udf(col("Commentaire_Client_sans_urls"))) \
    .withColumn("commentaire_moderateur_propre", supprimer_at_udf(col("commentaire_moderateur_sans_urls"))) \
    .withColumn("urls_apres", detecter_urls_udf(col("Commentaire_Client_propre"))) \
    .withColumn("at_apres", detecter_at_udf(col("Commentaire_Client_propre")))

# Forcer l'exÃ©cution des transformations
df_nettoye.cache().count()

temps_fin_nettoyage = time.time()
print(f"âœ… Nettoyage terminÃ© en {temps_fin_nettoyage - temps_debut_nettoyage:.2f} secondes")

# 8. STATISTIQUES APRÃˆS NETTOYAGE
avec_urls_apres = df_nettoye.filter(col("urls_apres") == 1).count()
avec_at_apres = df_nettoye.filter(col("at_apres") == 1).count()
supprimees_urls = avec_urls_avant - avec_urls_apres
supprimees_at = avec_at_avant - avec_at_apres
taux_urls = (supprimees_urls / avec_urls_avant * 100) if avec_urls_avant > 0 else 0
taux_at = (supprimees_at / avec_at_avant * 100) if avec_at_avant > 0 else 0

print(f"\nğŸ“Š STATISTIQUES APRÃˆS NETTOYAGE:")
print(f"   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
print(f"   â”‚ URLs avant            : {avec_urls_avant:<15} â”‚")
print(f"   â”‚ URLs aprÃ¨s            : {avec_urls_apres:<15} â”‚")
print(f"   â”‚ URLs supprimÃ©es       : {supprimees_urls:<15} â”‚")
print(f"   â”‚ Taux succÃ¨s URLs      : {taux_urls:<15.2f}% â”‚")
print(f"   â”‚ @ avant               : {avec_at_avant:<15} â”‚")
print(f"   â”‚ @ aprÃ¨s               : {avec_at_apres:<15} â”‚")
print(f"   â”‚ @ supprimÃ©s           : {supprimees_at:<15} â”‚")
print(f"   â”‚ Taux succÃ¨s @         : {taux_at:<15.2f}% â”‚")
print(f"   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")

# 9. STATISTIQUES PAR WORKER
print("\nğŸ“Š PERFORMANCE PAR WORKER:")
worker_perf = df_nettoye.groupBy("worker_name").agg(
    {"urls_avant": "sum", 
     "urls_apres": "sum",
     "at_avant": "sum",
     "at_apres": "sum",
     "Commentaire_Client": "count"}
).withColumnRenamed("sum(urls_avant)", "urls_trouvees") \
 .withColumnRenamed("sum(urls_apres)", "urls_restantes") \
 .withColumnRenamed("sum(at_avant)", "at_trouves") \
 .withColumnRenamed("sum(at_apres)", "at_restants") \
 .withColumnRenamed("count(Commentaire_Client)", "documents")

worker_perf.show()

# 10. PRÃ‰PARATION POUR MONGODB
print("\nğŸ’¾ PRÃ‰PARATION POUR SAUVEGARDE...")
temps_debut_preparation = time.time()

df_final = df_nettoye.select(
    "_id",
    col("Commentaire_Client_propre").alias("Commentaire_Client"),
    col("commentaire_moderateur_propre").alias("commentaire_moderateur"),
    "date",
    "source",
    "moderateur",
    "metadata",
    "statut"
)

temps_fin_preparation = time.time()
print(f"âœ… PrÃ©paration terminÃ©e en {temps_fin_preparation - temps_debut_preparation:.2f} secondes")

# 11. SAUVEGARDE DANS MONGODB
print("\nğŸ“ SAUVEGARDE DANS MONGODB...")
temps_debut_sauvegarde = time.time()

# Convertir en Pandas
print("   ğŸ”„ Conversion en Pandas...")
df_pandas = df_final.toPandas()
print(f"   âœ… {len(df_pandas)} lignes converties")

# Collection destination
collection_dest = db['commentaires_sans_urls_multinode2']
collection_dest.delete_many({})
print("   ğŸ§¹ Collection destination vidÃ©e")

# InsÃ©rer par lots
print("   ğŸ“¥ Insertion par lots...")
batch_size = 500
total_batches = (len(df_pandas) + batch_size - 1) // batch_size

for i in range(0, len(df_pandas), batch_size):
    batch_num = i//batch_size + 1
    batch = df_pandas.iloc[i:i+batch_size].to_dict('records')
    
    # Convertir les NaN en None pour MongoDB
    for doc in batch:
        for key, value in doc.items():
            if pd.isna(value):
                doc[key] = None
    
    collection_dest.insert_many(batch)
    print(f"   âœ“ Lot {batch_num}/{total_batches}: {len(batch)} documents")

temps_fin_sauvegarde = time.time()
print(f"\nâœ… {len(df_pandas)} documents sauvegardÃ©s dans 'commentaires_sans_urls_multinode2'")
print(f"   â±ï¸  Temps de sauvegarde: {temps_fin_sauvegarde - temps_debut_sauvegarde:.2f} secondes")

# 12. VÃ‰RIFICATION FINALE
print("\nğŸ” VÃ‰RIFICATION FINALE...")
temps_debut_verification = time.time()

# VÃ©rifier avec diffÃ©rents patterns
patterns_verif_urls = [
    r'https?://\S+',
    r'www\.\S+',
    r'https?://(?:\s|$)',
    r'https?://$'
]

print("\n   VÃ©rification URLs pattern par pattern:")
for pattern in patterns_verif_urls:
    count = collection_dest.count_documents({
        "Commentaire_Client": {"$regex": pattern, "$options": "i"}
    })
    print(f"   â€¢ {pattern[:20]}...: {count} documents")

# VÃ©rification des @
count_at = collection_dest.count_documents({
    "Commentaire_Client": {"$regex": "@", "$options": "i"}
})
print(f"\n   â€¢ VÃ©rification @: {count_at} documents avec @")

# VÃ©rification globale
urls_restantes = collection_dest.count_documents({
    "Commentaire_Client": {"$regex": "https?://|www\.", "$options": "i"}
})

temps_fin_verification = time.time()
print(f"\nğŸ“Š RÃ‰SULTAT FINAL (vÃ©rification en {temps_fin_verification - temps_debut_verification:.2f}s):")
print(f"   â€¢ Documents avec URLs restantes: {urls_restantes}")
print(f"   â€¢ Documents avec @ restants: {count_at}")
if urls_restantes == 0 and count_at == 0:
    print("   âœ… SUCCÃˆS : Toutes les URLs et tous les @ ont Ã©tÃ© supprimÃ©s !")
else:
    print(f"   âš ï¸ ATTENTION : {urls_restantes} URLs restantes, {count_at} @ restants")

# ğŸ FIN DU CHRONOMÃˆTRAGE GLOBAL
temps_fin_global = time.time()
temps_total = temps_fin_global - temps_debut_global

# 13. RAPPORT AVEC TEMPS D'EXÃ‰CUTION
print("\nğŸ“„ CRÃ‰ATION DU RAPPORT...")

rapport = f"""
{"="*70}
RAPPORT DE SUPPRESSION DES URLS ET DES @ - MODE MULTI-NODE
{"="*70}

Date d'exÃ©cution : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
Mode : Multi-node Spark Cluster

â±ï¸  TEMPS D'EXÃ‰CUTION:
   â€¢ Connexion Spark        : {temps_fin_spark - temps_debut_spark:.2f}s
   â€¢ Chargement donnÃ©es     : {temps_fin_chargement - temps_debut_chargement:.2f}s
   â€¢ Analyse des donnÃ©es    : {temps_fin_analyse - temps_debut_analyse:.2f}s
   â€¢ Nettoyage              : {temps_fin_nettoyage - temps_debut_nettoyage:.2f}s
   â€¢ PrÃ©paration DataFrame  : {temps_fin_preparation - temps_debut_preparation:.2f}s
   â€¢ Sauvegarde MongoDB     : {temps_fin_sauvegarde - temps_debut_sauvegarde:.2f}s
   â€¢ VÃ©rification           : {temps_fin_verification - temps_debut_verification:.2f}s
   â€¢ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   â€¢ TEMPS TOTAL            : {temps_total:.2f}s
   â€¢ Documents par seconde  : {total / temps_total:.2f} doc/s

ğŸ“Š STATISTIQUES GLOBALES:
   â€¢ Total documents traitÃ©s    : {total}
   
   ğŸ“ URLs:
   â€¢ Documents avec URLs (avant): {avec_urls_avant}
   â€¢ URLs dÃ©tectÃ©es (avant)     : {total_urls}
   â€¢ Documents avec URLs (aprÃ¨s): {avec_urls_apres}
   â€¢ URLs supprimÃ©es            : {supprimees_urls}
   â€¢ Taux de succÃ¨s URLs        : {taux_urls:.2f}%
   
   ğŸ“ @ (arobase):
   â€¢ Documents avec @ (avant)   : {avec_at_avant}
   â€¢ @ dÃ©tectÃ©s (avant)         : {total_at}
   â€¢ Documents avec @ (aprÃ¨s)   : {avec_at_apres}
   â€¢ @ supprimÃ©s                : {supprimees_at}
   â€¢ Taux de succÃ¨s @           : {taux_at:.2f}%

ğŸ“ STOCKAGE:
   â€¢ Collection source      : telecom_algerie.commentaires_bruts
   â€¢ Collection destination : telecom_algerie.commentaires_sans_urls_multinode2
   â€¢ Documents sauvegardÃ©s  : {len(df_pandas)}

ğŸ” VÃ‰RIFICATION FINALE:
   â€¢ URLs restantes dÃ©tectÃ©es : {urls_restantes}
   â€¢ @ restants dÃ©tectÃ©s      : {count_at}
   â€¢ Statut : {"âœ… SUCCÃˆS" if (urls_restantes == 0 and count_at == 0) else "âš ï¸ Ã‰CHEC"}

âš¡ DISTRIBUTION:
   â€¢ Workers utilisÃ©s : {df_with_workers.select("worker_name").distinct().count()}
"""

# Sauvegarder le rapport
os.makedirs("donnees/resultats", exist_ok=True)
rapport_path = "donnees/resultats/rapport_urls_multinode2.txt"
with open(rapport_path, "w", encoding="utf-8") as f:
    f.write(rapport)
print(f"âœ… Rapport sauvegardÃ©: {rapport_path}")

# 14. RÃ‰SUMÃ‰ FINAL AVEC TEMPS
print("\n" + "="*70)
print("ğŸ“Š RÃ‰SUMÃ‰ FINAL - MODE MULTI-NODE")
print("="*70)
print(f"ğŸ“¥ Documents traitÃ©s    : {total}")
print(f"\nğŸ“ URLs:")
print(f"   â€¢ DÃ©tectÃ©es : {total_urls}")
print(f"   â€¢ SupprimÃ©es: {supprimees_urls}")
print(f"   â€¢ Taux      : {taux_urls:.2f}%")
print(f"\nğŸ“ @ (arobase):")
print(f"   â€¢ DÃ©tectÃ©s  : {total_at}")
print(f"   â€¢ SupprimÃ©s : {supprimees_at}")
print(f"   â€¢ Taux      : {taux_at:.2f}%")
print(f"\nâ±ï¸  TEMPS D'EXÃ‰CUTION:")
print(f"   â€¢ Chargement : {temps_fin_chargement - temps_debut_chargement:.2f}s")
print(f"   â€¢ Nettoyage  : {temps_fin_nettoyage - temps_debut_nettoyage:.2f}s")
print(f"   â€¢ Sauvegarde : {temps_fin_sauvegarde - temps_debut_sauvegarde:.2f}s")
print(f"   â€¢ TOTAL      : {temps_total:.2f}s")
print(f"   â€¢ Vitesse    : {total / temps_total:.2f} docs/s")
print(f"\nğŸ“ Collection MongoDB:")
print(f"   â€¢ telecom_algerie.commentaires_sans_urls_multinode2")
print("="*70)

print("\nğŸ‰ SUPPRESSION DES URLS ET DES @ TERMINÃ‰E EN MODE MULTI-NODE !")

# Fermer les connexions
spark.stop()
client.close()
print("\nğŸ”Œ Connexions fermÃ©es")