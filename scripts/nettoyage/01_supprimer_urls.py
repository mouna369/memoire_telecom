# # # # scripts/nettoyage/01_supprimer_urls.py - VERSION AVEC CSV
# # # from pyspark.sql import SparkSession
# # # from pyspark.sql.functions import col, regexp_replace, when
# # # import pandas as pd
# # # import re

# # # def supprimer_urls(texte):
# # #     """Supprime les URLs d'un texte"""
# # #     if pd.isna(texte):
# # #         return ""
# # #     texte = str(texte)
# # #     pattern = r'http[s]?://\S+|www\.\S+'
# # #     return re.sub(pattern, '', texte)

# # # print("="*60)
# # # print("ğŸ” Ã‰TAPE 1 : DÃ‰TECTION ET SUPPRESSION DES URLS")
# # # print("="*60)

# # # # 1. CrÃ©er Spark
# # # spark = SparkSession.builder \
# # #     .appName("Suppression_URLs") \
# # #     .master("local[*]") \
# # #     .getOrCreate()
# # # print("âœ… Spark dÃ©marrÃ©")

# # # # 2. Charger les donnÃ©es depuis l'Excel directement
# # # print("\nğŸ“‚ Chargement depuis Excel...")
# # # pandas_df = pd.read_excel("donnees/brutes/Social-Media-Analytics1.xlsx", header=1)
# # # print(f"âœ… {len(pandas_df)} commentaires chargÃ©s")

# # # # 3. Identifier la colonne des commentaires
# # # colonne_commentaire = None
# # # for col_name in pandas_df.columns:
# # #     if 'commentaire' in str(col_name).lower():
# # #         colonne_commentaire = col_name
# # #         break

# # # if colonne_commentaire is None:
# # #     colonne_commentaire = pandas_df.columns[0]
# # #     print(f"âš ï¸ Colonne non trouvÃ©e, utilisation de: {colonne_commentaire}")
# # # else:
# # #     print(f"ğŸ“‹ Colonne analysÃ©e : {colonne_commentaire}")

# # # # 4. ANALYSE : DÃ©tecter les URLs
# # # print("\nğŸ” ANALYSE : Recherche des URLs...")

# # # # Fonction pour dÃ©tecter les URLs
# # # def detecter_urls_texte(texte):
# # #     if pd.isna(texte):
# # #         return 0, ""
# # #     texte = str(texte)
# # #     pattern = r'http[s]?://\S+|www\.\S+'
# # #     urls = re.findall(pattern, texte)
# # #     return len(urls), " | ".join(urls)

# # # # Appliquer la dÃ©tection
# # # urls_info = pandas_df[colonne_commentaire].apply(detecter_urls_texte)
# # # pandas_df['nb_urls'] = urls_info.apply(lambda x: x[0])
# # # pandas_df['urls_trouvees'] = urls_info.apply(lambda x: x[1])

# # # # Compter les commentaires avec URLs
# # # nb_total = len(pandas_df)
# # # nb_avec_urls = (pandas_df['nb_urls'] > 0).sum()
# # # nb_sans_urls = nb_total - nb_avec_urls
# # # pourcentage = (nb_avec_urls / nb_total * 100) if nb_total > 0 else 0

# # # print(f"\nğŸ“Š STATISTIQUES DES URLS:")
# # # print(f"   - Total commentaires : {nb_total}")
# # # print(f"   - Commentaires avec URLs : {nb_avec_urls} ({pourcentage:.2f}%)")
# # # print(f"   - Commentaires sans URLs : {nb_sans_urls}")

# # # # 5. AFFICHER les commentaires avec URLs
# # # if nb_avec_urls > 0:
# # #     print(f"\nğŸ“ COMMENTAIRES AVEC URLS TROUVÃ‰S:")
# # #     df_urls = pandas_df[pandas_df['nb_urls'] > 0]
    
# # #     for idx, row in df_urls.head(10).iterrows():
# # #         print(f"\n   Ligne {idx + 2}:")
# # #         print(f"   Texte: {row[colonne_commentaire][:100]}...")
# # #         print(f"   URLs: {row['urls_trouvees']}")

# # # # 6. SUPPRESSION des URLs
# # # print("\nğŸ§¹ SUPPRESSION DES URLS...")
# # # pandas_df['commentaire_sans_urls'] = pandas_df[colonne_commentaire].apply(supprimer_urls)

# # # # 7. VÃ‰RIFICATION
# # # print("ğŸ” VÃ‰RIFICATION...")

# # # # Compter s'il reste des URLs
# # # pandas_df['verification_urls'] = pandas_df['commentaire_sans_urls'].apply(
# # #     lambda x: len(re.findall(r'http[s]?://\S+|www\.\S+', str(x)))
# # # )
# # # nb_reste = (pandas_df['verification_urls'] > 0).sum()

# # # if nb_reste == 0:
# # #     print("âœ… SUCCÃˆS : Toutes les URLs ont Ã©tÃ© supprimÃ©es !")
# # # else:
# # #     print(f"âš ï¸ ATTENTION : Il reste {nb_reste} commentaires avec URLs")

# # # # 8. CRÃ‰ER LE FICHIER FINAL (MÃŠME STRUCTURE QUE L'ORIGINAL)
# # # print("\nğŸ’¾ CrÃ©ation du fichier CSV final...")

# # # # Garder les mÃªmes colonnes que l'original + la version nettoyÃ©e
# # # colonnes_a_garder = list(pandas_df.columns)
# # # # Enlever les colonnes temporaires
# # # colonnes_temp = ['nb_urls', 'urls_trouvees', 'verification_urls']
# # # colonnes_finales = [c for c in colonnes_a_garder if c not in colonnes_temp]

# # # # CrÃ©er le DataFrame final avec la colonne originale remplacÃ©e par la version nettoyÃ©e
# # # df_final = pandas_df.copy()
# # # df_final[colonne_commentaire] = df_final['commentaire_sans_urls']
# # # df_final = df_final[colonnes_finales]

# # # # Sauvegarder en CSV
# # # csv_path = "donnees/resultats/donnees_sans_urls.csv"
# # # df_final.to_csv(csv_path, index=False, encoding='utf-8-sig')
# # # print(f"âœ… Fichier CSV crÃ©Ã© : {csv_path}")

# # # # 9. CRÃ‰ER UN RAPPORT
# # # print("\nğŸ“„ CrÃ©ation du rapport...")
# # # with open("donnees/resultats/rapport_urls.txt", "w", encoding="utf-8") as f:
# # #     f.write("="*60 + "\n")
# # #     f.write("RAPPORT DE DÃ‰TECTION ET SUPPRESSION DES URLS\n")
# # #     f.write("="*60 + "\n\n")
# # #     f.write(f"Date : 2024-02-23\n")
# # #     f.write(f"Fichier source : Social-Media-Analytics.xlsx\n")
# # #     f.write(f"Total commentaires : {nb_total}\n")
# # #     f.write(f"Commentaires avec URLs : {nb_avec_urls}\n")
# # #     f.write(f"Pourcentage : {pourcentage:.2f}%\n")
# # #     f.write(f"URLs supprimÃ©es avec succÃ¨s : {'OUI' if nb_reste==0 else 'NON'}\n")
# # #     f.write(f"\nFichier crÃ©Ã© : donnees_sans_urls.csv\n")

# # # print("âœ… Rapport sauvegardÃ©")

# # # # 10. EXEMPLES AVANT/APRÃˆS
# # # print("\nğŸ“Š EXEMPLES AVANT/APRÃˆS SUPPRESSION:")
# # # if nb_avec_urls > 0:
# # #     exemples = pandas_df[pandas_df['nb_urls'] > 0].head(3)
# # #     for idx, row in exemples.iterrows():
# # #         print(f"\n   AVANT: {row[colonne_commentaire][:100]}...")
# # #         print(f"   APRÃˆS: {row['commentaire_sans_urls'][:100]}...")
# # # else:
# # #     print("   Aucun exemple avec URLs")

# # # print("\n" + "="*60)
# # # print("ğŸ“Š RÃ‰SUMÃ‰ FINAL")
# # # print("="*60)
# # # print(f"âœ… {nb_avec_urls} commentaires avec URLs ont Ã©tÃ© traitÃ©s")
# # # print(f"âœ… Fichier crÃ©Ã© : donnees/resultats/donnees_sans_urls.csv")
# # # print(f"âœ… Tu peux l'ouvrir avec Excel ou n'importe quel Ã©diteur")
# # # print("="*60)

# # # print("\nğŸ‰ Ã‰TAPE 1 TERMINÃ‰E !")


# # #!/usr/bin/env python3
# # # -*- coding: utf-8 -*-

# # # scripts/nettoyage/01_supprimer_urls.py - VERSION AVEC STOCKAGE MONGODB

# # from pyspark.sql import SparkSession
# # from pyspark.sql.functions import col, udf
# # from pyspark.sql.types import StringType, IntegerType
# # import pandas as pd
# # import re
# # import os
# # from pymongo import MongoClient
# # from datetime import datetime
# # from bson import ObjectId

# # def supprimer_urls(texte):
# #     """Supprime les URLs d'un texte"""
# #     if texte is None or not isinstance(texte, str):
# #         return texte
# #     # Pattern pour dÃ©tecter les URLs
# #     pattern = r'http[s]?://\S+|www\.\S+'
# #     texte_propre = re.sub(pattern, '', texte)
# #     # Supprimer les espaces multiples
# #     texte_propre = re.sub(r'\s+', ' ', texte_propre).strip()
# #     return texte_propre if texte_propre else None

# # def detecter_urls(texte):
# #     """DÃ©tecte si un texte contient des URLs"""
# #     if texte is None or not isinstance(texte, str):
# #         return 0
# #     pattern = r'http[s]?://\S+|www\.\S+'
# #     return 1 if re.search(pattern, texte) else 0

# # def extraire_urls(texte):
# #     """Extrait toutes les URLs d'un texte"""
# #     if texte is None or not isinstance(texte, str):
# #         return []
# #     pattern = r'http[s]?://\S+|www\.\S+'
# #     return re.findall(pattern, texte)

# # print("="*70)
# # print("ğŸ” Ã‰TAPE 1 : DÃ‰TECTION ET SUPPRESSION DES URLS")
# # print("="*70)

# # # 1. CrÃ©er Spark
# # print("\nâš¡ DÃ©marrage de Spark...")
# # spark = SparkSession.builder \
# #     .appName("Suppression_URLs") \
# #     .master("local[*]") \
# #     .config("spark.executor.memory", "4g") \
# #     .config("spark.driver.memory", "4g") \
# #     .getOrCreate()
# # print("âœ… Spark dÃ©marrÃ©")

# # # 2. Connexion Ã  MongoDB
# # print("\nğŸ“‚ Connexion Ã  MongoDB...")
# # try:
# #     client = MongoClient('localhost', 27018)
# #     db = client['telecom_algerie']
    
# #     # Collection source
# #     collection_source = db['commentaires_bruts']
    
# #     # Collection destination (nettoyÃ©e)
# #     collection_dest = db['commentaires_sans_urls']
    
# #     # Vider la collection de destination si elle existe
# #     collection_dest.delete_many({})
    
# #     print("âœ… Connexion MongoDB rÃ©ussie")
    
# # except Exception as e:
# #     print(f"âŒ Erreur de connexion MongoDB: {e}")
# #     spark.stop()
# #     exit(1)

# # # 3. Charger les donnÃ©es
# # print("\nğŸ“¥ Chargement des commentaires...")
# # data = list(collection_source.find({}))
# # print(f"ğŸ“Š {len(data)} commentaires chargÃ©s")

# # if len(data) == 0:
# #     print("âŒ Aucune donnÃ©e trouvÃ©e")
# #     spark.stop()
# #     exit(1)

# # # 4. ANALYSE : DÃ©tecter les URLs
# # print("\nğŸ” ANALYSE : Recherche des URLs...")

# # total_avec_urls = 0
# # total_urls_trouvees = 0
# # exemples_urls = []

# # for doc in data[:10]:  # Seulement pour l'affichage des exemples
# #     commentaire = doc.get('Commentaire_Client', '')
# #     urls = extraire_urls(commentaire)
# #     if urls:
# #         exemples_urls.append({
# #             'texte': commentaire[:150],
# #             'urls': urls
# #         })

# # # Compter tous les URLs
# # for doc in data:
# #     commentaire = doc.get('Commentaire_Client', '')
# #     urls = extraire_urls(commentaire)
# #     if urls:
# #         total_avec_urls += 1
# #         total_urls_trouvees += len(urls)

# # print(f"\nğŸ“Š STATISTIQUES:")
# # print(f"   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
# # print(f"   â”‚ Total commentaires    : {len(data):<15} â”‚")
# # print(f"   â”‚ Avec URLs             : {total_avec_urls:<15} â”‚")
# # print(f"   â”‚ URLs trouvÃ©es         : {total_urls_trouvees:<15} â”‚")
# # print(f"   â”‚ Pourcentage           : {(total_avec_urls/len(data)*100):<15.2f}% â”‚")
# # print(f"   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")

# # # Afficher des exemples
# # if exemples_urls:
# #     print("\nğŸ“ EXEMPLES DE COMMENTAIRES AVEC URLS:")
# #     for i, ex in enumerate(exemples_urls[:5], 1):
# #         print(f"\n   Exemple {i}:")
# #         print(f"   ğŸ“ Texte: {ex['texte']}...")
# #         print(f"   ğŸ”— URLs: {', '.join(ex['urls'])}")
# #         print("   " + "-" * 60)

# # # 5. NETTOYAGE : Supprimer les URLs
# # print("\nğŸ§¹ SUPPRESSION DES URLS EN COURS...")

# # docs_nettoyes = []
# # docs_avec_modifications = 0

# # for i, doc in enumerate(data):
# #     # CrÃ©er une copie du document
# #     doc_propre = doc.copy()
    
# #     # Nettoyer le commentaire client
# #     commentaire_original = doc.get('Commentaire_Client', '')
# #     commentaire_nettoye = supprimer_urls(commentaire_original)
    
# #     if commentaire_original != commentaire_nettoye:
# #         docs_avec_modifications += 1
    
# #     doc_propre['Commentaire_Client'] = commentaire_nettoye
    
# #     # Nettoyer le commentaire moderateur s'il existe
# #     if 'commentaire_moderateur' in doc:
# #         mod_original = doc.get('commentaire_moderateur', '')
# #         mod_nettoye = supprimer_urls(mod_original)
# #         doc_propre['commentaire_moderateur'] = mod_nettoye
    
# #     # Ajouter des mÃ©tadonnÃ©es de nettoyage
# #     doc_propre['_nettoyage'] = {
# #         'date_nettoyage': datetime.now(),
# #         'etape': 'suppression_urls',
# #         'urls_supprimees': len(extraire_urls(commentaire_original)) > 0,
# #         'nb_urls_trouvees': len(extraire_urls(commentaire_original))
# #     }
    
# #     docs_nettoyes.append(doc_propre)
    
# #     # Afficher la progression
# #     if (i + 1) % 5000 == 0:
# #         print(f"   âœ“ {i + 1}/{len(data)} documents traitÃ©s")

# # print(f"\nâœ… Traitement terminÃ©: {len(docs_nettoyes)} documents")
# # print(f"   â€¢ Documents modifiÃ©s: {docs_avec_modifications}")

# # # 6. SAUVEGARDE DANS MONGODB
# # print("\nğŸ’¾ SAUVEGARDE DANS MongoDB...")

# # try:
# #     # InsÃ©rer par lots de 1000 pour Ã©viter les timeout
# #     batch_size = 1000
# #     for i in range(0, len(docs_nettoyes), batch_size):
# #         batch = docs_nettoyes[i:i+batch_size]
# #         collection_dest.insert_many(batch)
# #         print(f"   âœ“ Lot {i//batch_size + 1}: {len(batch)} documents sauvegardÃ©s")
    
# #     print(f"\nâœ… {len(docs_nettoyes)} documents sauvegardÃ©s dans 'commentaires_sans_urls'")
    
# # except Exception as e:
# #     print(f"âŒ Erreur lors de la sauvegarde: {e}")

# # # 7. VÃ‰RIFICATION
# # print("\nğŸ” VÃ‰RIFICATION DE LA SUPPRESSION...")

# # # VÃ©rifier dans la nouvelle collection
# # echantillon = collection_dest.find().limit(5)
# # urls_restantes = 0
# # total_verif = collection_dest.count_documents({})

# # print(f"\nğŸ“Š VÃ©rification sur {total_verif} documents:")

# # # VÃ©rifier quelques documents
# # for doc in collection_dest.find().limit(20):
# #     commentaire = doc.get('Commentaire_Client', '')
# #     if commentaire and extraire_urls(commentaire):
# #         urls_restantes += 1
# #         print(f"   âš ï¸ URL restante trouvÃ©e: {commentaire[:100]}...")

# # if urls_restantes == 0:
# #     print("   âœ… Aucune URL restante dans l'Ã©chantillon vÃ©rifiÃ©")
# # else:
# #     print(f"   âš ï¸ {urls_restantes} URLs restantes trouvÃ©es")

# # # 8. CRÃ‰ER UN RAPPORT
# # print("\nğŸ“„ CRÃ‰ATION DU RAPPORT...")

# # rapport = f"""
# # {"="*70}
# # RAPPORT DE SUPPRESSION DES URLS
# # {"="*70}

# # Date d'exÃ©cution : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

# # ğŸ“Š STATISTIQUES:
# #    â€¢ Total commentaires traitÃ©s : {len(data)}
# #    â€¢ Commentaires avec URLs     : {total_avec_urls}
# #    â€¢ URLs trouvÃ©es              : {total_urls_trouvees}
# #    â€¢ Pourcentage avec URLs      : {total_avec_urls/len(data)*100:.2f}%
# #    â€¢ Documents modifiÃ©s         : {docs_avec_modifications}

# # ğŸ“ STOCKAGE:
# #    â€¢ Collection source      : telecom_algerie.commentaires_bruts
# #    â€¢ Collection destination : telecom_algerie.commentaires_sans_urls
# #    â€¢ Documents sauvegardÃ©s  : {len(docs_nettoyes)}

# # ğŸ” EXEMPLES D'URLS SUPPRIMÃ‰ES:
# # """

# # # Ajouter des exemples au rapport
# # for i, ex in enumerate(exemples_urls[:5]):
# #     rapport += f"\n   {i+1}. URLs: {', '.join(ex['urls'])}"
# #     rapport += f"\n      Texte: {ex['texte'][:100]}...\n"

# # # Sauvegarder le rapport
# # rapport_path = "donnees/resultats/rapport_urls.txt"
# # os.makedirs("donnees/resultats", exist_ok=True)
# # with open(rapport_path, "w", encoding="utf-8") as f:
# #     f.write(rapport)

# # print(f"âœ… Rapport sauvegardÃ©: {rapport_path}")

# # # 9. EXPORT OPTIONNEL EN CSV/EXCEL (si vraiment nÃ©cessaire)
# # print("\nğŸ“ EXPORT OPTIONNEL EN CSV/EXCEL...")

# # reponse = input("\nVoulez-vous aussi exporter en CSV/Excel ? (o/n): ")
# # if reponse.lower() == 'o':
# #     try:
# #         # RÃ©cupÃ©rer quelques documents pour l'export
# #         docs_export = list(collection_dest.find().limit(1000))
        
# #         # Convertir ObjectId en string
# #         for doc in docs_export:
# #             doc['_id'] = str(doc['_id'])
# #             if '_nettoyage' in doc:
# #                 doc['_nettoyage'] = str(doc['_nettoyage'])
        
# #         # CrÃ©er DataFrame
# #         df_export = pd.DataFrame(docs_export)
        
# #         # Exporter
# #         csv_path = "donnees/resultats/commentaires_sans_urls.csv"
# #         excel_path = "donnees/resultats/commentaires_sans_urls.xlsx"
        
# #         df_export.to_csv(csv_path, index=False, encoding='utf-8-sig')
# #         df_export.to_excel(excel_path, index=False)
        
# #         print(f"âœ… CSV exportÃ©: {csv_path} (1000 premiÃ¨res lignes)")
# #         print(f"âœ… Excel exportÃ©: {excel_path} (1000 premiÃ¨res lignes)")
        
# #     except Exception as e:
# #         print(f"âŒ Erreur lors de l'export: {e}")

# # # 10. RÃ‰SUMÃ‰ FINAL
# # print("\n" + "="*70)
# # print("ğŸ“Š RÃ‰SUMÃ‰ FINAL")
# # print("="*70)
# # print(f"ğŸ“¥ Commentaires traitÃ©s    : {len(data)}")
# # print(f"ğŸ”— URLs dÃ©tectÃ©es          : {total_urls_trouvees}")
# # print(f"ğŸ“ Commentaires avec URLs  : {total_avec_urls}")
# # print(f"âœ… Documents modifiÃ©s      : {docs_avec_modifications}")
# # print(f"\nğŸ“ Base de donnÃ©es MongoDB:")
# # print(f"   â€¢ Source : telecom_algerie.commentaires_bruts")
# # print(f"   â€¢ Destination : telecom_algerie.commentaires_sans_urls")
# # print("="*70)

# # print("\nğŸ‰ Ã‰TAPE 1 TERMINÃ‰E AVEC SUCCÃˆS !")
# # print(f"ğŸ’¡ Les commentaires nettoyÃ©s sont dans: telecom_algerie.commentaires_sans_urls")

# # # Fermer les connexions
# # spark.stop()
# # client.close()
# # print("\nğŸ”Œ Connexions fermÃ©es")

# #!/usr/bin/env python3
# # -*- coding: utf-8 -*-

# # scripts/nettoyage/01_supprimer_urls.py - VERSION CORRIGÃ‰E AVEC PATTERNS AMÃ‰LIORÃ‰S

# from pyspark.sql import SparkSession
# from pyspark.sql.functions import col, udf
# from pyspark.sql.types import StringType, IntegerType
# import pandas as pd
# import re
# import os
# from pymongo import MongoClient
# from datetime import datetime
# from bson import ObjectId

# def supprimer_urls(texte):
#     """Supprime les URLs d'un texte - Version amÃ©liorÃ©e"""
#     if texte is None or not isinstance(texte, str):
#         return texte
    
#     # PATTERNS AMÃ‰LIORÃ‰S pour dÃ©tecter tous les types d'URLs
#     patterns = [
#         r'https?://\S+',           # URLs complÃ¨tes (http://example.com)
#         r'www\.\S+',                # www.example.com
#         r'https?://(?:\s|$)',       # https:// seul suivi d'espace ou fin de ligne
#         r'https?://$',              # https:// en fin de chaÃ®ne
#         r'\bhttps?://\b',           # https:// comme mot isolÃ©
#         r'http://(?:\s|$)',         # http:// seul
#         r'http://$'                 # http:// en fin de chaÃ®ne
#     ]
    
#     texte_propre = texte
#     for pattern in patterns:
#         texte_propre = re.sub(pattern, '', texte_propre, flags=re.IGNORECASE)
    
#     # Supprimer les espaces multiples
#     texte_propre = re.sub(r'\s+', ' ', texte_propre).strip()
#     return texte_propre if texte_propre else None

# def detecter_urls(texte):
#     """DÃ©tecte si un texte contient des URLs - Version amÃ©liorÃ©e"""
#     if texte is None or not isinstance(texte, str):
#         return 0
    
#     # Patterns de dÃ©tection
#     patterns = [
#         r'https?://',
#         r'www\.',
#         r'https?://(?:\s|$)',
#         r'https?://$'
#     ]
    
#     for pattern in patterns:
#         if re.search(pattern, texte, re.IGNORECASE):
#             return 1
#     return 0

# def extraire_urls(texte):
#     """Extrait toutes les URLs d'un texte - Version amÃ©liorÃ©e"""
#     if texte is None or not isinstance(texte, str):
#         return []
    
#     patterns = [
#         r'https?://\S+',
#         r'www\.\S+',
#         r'https?://(?:\s|$)',
#         r'https?://$'
#     ]
    
#     urls = []
#     for pattern in patterns:
#         found = re.findall(pattern, texte, re.IGNORECASE)
#         urls.extend([u for u in found if u.strip()])  # Ã‰viter les chaÃ®nes vides
    
#     return list(set(urls))  # Ã‰liminer les doublons

# def compter_urls(texte):
#     """Compte le nombre d'URLs dans un texte"""
#     if texte is None or not isinstance(texte, str):
#         return 0
#     return len(extraire_urls(texte))

# print("="*70)
# print("ğŸ” Ã‰TAPE 1 : DÃ‰TECTION ET SUPPRESSION DES URLS")
# print("="*70)

# # 1. CrÃ©er Spark
# print("\nâš¡ DÃ©marrage de Spark...")
# spark = SparkSession.builder \
#     .appName("Suppression_URLs") \
#     .master("local[*]") \
#     .config("spark.executor.memory", "4g") \
#     .config("spark.driver.memory", "4g") \
#     .getOrCreate()
# print("âœ… Spark dÃ©marrÃ©")

# # 2. Connexion Ã  MongoDB
# print("\nğŸ“‚ Connexion Ã  MongoDB...")
# try:
#     client = MongoClient('localhost', 27018)
#     db = client['telecom_algerie']
    
#     # Collection source
#     collection_source = db['commentaires_bruts']
    
#     # Collection destination (nettoyÃ©e)
#     collection_dest = db['commentaires_sans_urls_v2']
    
#     # Vider la collection de destination si elle existe
#     collection_dest.delete_many({})
    
#     print("âœ… Connexion MongoDB rÃ©ussie")
    
# except Exception as e:
#     print(f"âŒ Erreur de connexion MongoDB: {e}")
#     spark.stop()
#     exit(1)

# # 3. Charger les donnÃ©es
# print("\nğŸ“¥ Chargement des commentaires...")
# data = list(collection_source.find({}))
# print(f"ğŸ“Š {len(data)} commentaires chargÃ©s")

# if len(data) == 0:
#     print("âŒ Aucune donnÃ©e trouvÃ©e")
#     spark.stop()
#     exit(1)

# # 4. ANALYSE : DÃ©tecter les URLs (avec nouveaux patterns)
# print("\nğŸ” ANALYSE : Recherche des URLs (version amÃ©liorÃ©e)...")

# total_avec_urls = 0
# total_urls_trouvees = 0
# exemples_urls = []
# cas_speciaux = []  # Pour capturer les cas comme "https://" seul

# for doc in data[:50]:  # Analyser plus d'exemples pour trouver les cas spÃ©ciaux
#     commentaire = doc.get('Commentaire_Client', '')
#     urls = extraire_urls(commentaire)
    
#     # VÃ©rifier spÃ©cifiquement les cas "https://" seul
#     if re.search(r'https?://(?:\s|$)', commentaire, re.IGNORECASE):
#         cas_speciaux.append({
#             'texte': commentaire[:150],
#             'urls': urls
#         })
    
#     if urls:
#         exemples_urls.append({
#             'texte': commentaire[:150],
#             'urls': urls
#         })

# # Compter tous les URLs
# for doc in data:
#     commentaire = doc.get('Commentaire_Client', '')
#     urls = extraire_urls(commentaire)
#     if urls:
#         total_avec_urls += 1
#         total_urls_trouvees += len(urls)

# print(f"\nğŸ“Š STATISTIQUES:")
# print(f"   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
# print(f"   â”‚ Total commentaires    : {len(data):<15} â”‚")
# print(f"   â”‚ Avec URLs             : {total_avec_urls:<15} â”‚")
# print(f"   â”‚ URLs trouvÃ©es         : {total_urls_trouvees:<15} â”‚")
# print(f"   â”‚ Pourcentage           : {(total_avec_urls/len(data)*100):<15.2f}% â”‚")
# print(f"   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")

# # Afficher les cas spÃ©ciaux (https:// seul)
# if cas_speciaux:
#     print(f"\nâš ï¸ CAS SPÃ‰CIAUX DÃ‰TECTÃ‰S (https:// seul):")
#     for i, ex in enumerate(cas_speciaux[:3], 1):
#         print(f"\n   Cas {i}:")
#         print(f"   ğŸ“ Texte: {ex['texte']}")
#         print(f"   ğŸ”— URLs: {', '.join(ex['urls'])}")
#         print("   " + "-" * 60)

# # Afficher des exemples gÃ©nÃ©raux
# if exemples_urls:
#     print("\nğŸ“ EXEMPLES DE COMMENTAIRES AVEC URLS:")
#     for i, ex in enumerate(exemples_urls[:5], 1):
#         print(f"\n   Exemple {i}:")
#         print(f"   ğŸ“ Texte: {ex['texte']}...")
#         print(f"   ğŸ”— URLs: {', '.join(ex['urls'])}")
#         print("   " + "-" * 60)

# # 5. NETTOYAGE : Supprimer les URLs
# print("\nğŸ§¹ SUPPRESSION DES URLS EN COURS (version amÃ©liorÃ©e)...")

# docs_nettoyes = []
# docs_avec_modifications = 0
# urls_par_doc = []

# for i, doc in enumerate(data):
#     # CrÃ©er une copie du document
#     doc_propre = doc.copy()
    
#     # Nettoyer le commentaire client
#     commentaire_original = doc.get('Commentaire_Client', '')
#     commentaire_nettoye = supprimer_urls(commentaire_original)
    
#     # Compter les URLs avant/aprÃ¨s
#     urls_avant = compter_urls(commentaire_original)
#     urls_apres = compter_urls(commentaire_nettoye)
    
#     if urls_avant > 0:
#         urls_par_doc.append({
#             'id': doc.get('_id'),
#             'avant': urls_avant,
#             'apres': urls_apres,
#             'texte': commentaire_original[:100]
#         })
    
#     if commentaire_original != commentaire_nettoye:
#         docs_avec_modifications += 1
    
#     doc_propre['Commentaire_Client'] = commentaire_nettoye
    
#     # Nettoyer le commentaire moderateur s'il existe
#     if 'commentaire_moderateur' in doc:
#         mod_original = doc.get('commentaire_moderateur', '')
#         mod_nettoye = supprimer_urls(mod_original)
#         doc_propre['commentaire_moderateur'] = mod_nettoye
    
#     # Ajouter des mÃ©tadonnÃ©es de nettoyage dÃ©taillÃ©es
#     doc_propre['_nettoyage'] = {
#         'date_nettoyage': datetime.now(),
#         'etape': 'suppression_urls_v2',
#         'urls_avant': urls_avant,
#         'urls_apres': urls_apres,
#         'urls_supprimees': urls_avant > 0
#     }
    
#     docs_nettoyes.append(doc_propre)
    
#     # Afficher la progression
#     if (i + 1) % 5000 == 0:
#         print(f"   âœ“ {i + 1}/{len(data)} documents traitÃ©s")

# print(f"\nâœ… Traitement terminÃ©: {len(docs_nettoyes)} documents")
# print(f"   â€¢ Documents modifiÃ©s: {docs_avec_modifications}")

# # Afficher quelques statistiques sur les URLs par document
# if urls_par_doc:
#     print("\nğŸ“Š DÃ©tail des URLs par document (Ã©chantillon):")
#     for item in urls_par_doc[:5]:
#         print(f"   â€¢ Document {item['id']}: {item['avant']} URLs â†’ {item['apres']} aprÃ¨s")
#         print(f"     Texte: {item['texte']}...")

# # 6. SAUVEGARDE DANS MONGODB
# print("\nğŸ’¾ SAUVEGARDE DANS MongoDB...")

# try:
#     # InsÃ©rer par lots de 1000
#     batch_size = 1000
#     for i in range(0, len(docs_nettoyes), batch_size):
#         batch = docs_nettoyes[i:i+batch_size]
#         collection_dest.insert_many(batch)
#         print(f"   âœ“ Lot {i//batch_size + 1}: {len(batch)} documents sauvegardÃ©s")
    
#     print(f"\nâœ… {len(docs_nettoyes)} documents sauvegardÃ©s dans 'commentaires_sans_urls_v2'")
    
# except Exception as e:
#     print(f"âŒ Erreur lors de la sauvegarde: {e}")

# # 7. VÃ‰RIFICATION APPROFONDIE
# print("\nğŸ” VÃ‰RIFICATION APPROFONDIE DE LA SUPPRESSION...")

# # VÃ©rifier avec diffÃ©rents patterns
# patterns_verification = [
#     r'https?://\S+',
#     r'www\.\S+',
#     r'https?://(?:\s|$)',
#     r'https?://$'
# ]

# print("\nğŸ“Š VÃ©rification pattern par pattern:")

# for pattern in patterns_verification:
#     count = collection_dest.count_documents({
#         "Commentaire_Client": {"$regex": pattern, "$options": "i"}
#     })
#     print(f"   â€¢ Pattern '{pattern[:20]}...': {count} documents")

# # VÃ©rification globale
# urls_restantes = collection_dest.count_documents({
#     "Commentaire_Client": {"$regex": "https?://|www\.", "$options": "i"}
# })

# print(f"\nğŸ“Š RÃ‰SULTAT DE LA VÃ‰RIFICATION:")
# print(f"   â€¢ Total documents avec URLs restantes: {urls_restantes}")

# if urls_restantes == 0:
#     print("   âœ… SUCCÃˆS : Aucune URL restante dÃ©tectÃ©e !")
# else:
#     print(f"   âš ï¸ ATTENTION : {urls_restantes} documents ont encore des URLs")
    
#     # Afficher les documents problÃ©matiques
#     print("\nğŸ“ DOCUMENTS AVEC URLS RESTANTES:")
#     docs_problemes = collection_dest.find({
#         "Commentaire_Client": {"$regex": "https?://|www\.", "$options": "i"}
#     }).limit(5)
    
#     for doc in docs_problemes:
#         print(f"\n   â€¢ ID: {doc['_id']}")
#         print(f"     Texte: {doc.get('Commentaire_Client', '')[:150]}...")

# # 8. CRÃ‰ER UN RAPPORT DÃ‰TAILLÃ‰
# print("\nğŸ“„ CRÃ‰ATION DU RAPPORT...")

# rapport = f"""
# {"="*70}
# RAPPORT DE SUPPRESSION DES URLS - VERSION AMÃ‰LIORÃ‰E
# {"="*70}

# Date d'exÃ©cution : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

# ğŸ“Š STATISTIQUES GLOBALES:
#    â€¢ Total commentaires traitÃ©s : {len(data)}
#    â€¢ Commentaires avec URLs     : {total_avec_urls}
#    â€¢ URLs trouvÃ©es              : {total_urls_trouvees}
#    â€¢ Pourcentage avec URLs      : {total_avec_urls/len(data)*100:.2f}%
#    â€¢ Documents modifiÃ©s         : {docs_avec_modifications}

# ğŸ“ STOCKAGE:
#    â€¢ Collection source      : telecom_algerie.commentaires_bruts
#    â€¢ Collection destination : telecom_algerie.commentaires_sans_urls
#    â€¢ Documents sauvegardÃ©s  : {len(docs_nettoyes)}

# ğŸ” RÃ‰SULTATS DE LA VÃ‰RIFICATION:
#    â€¢ Documents avec URLs restantes : {urls_restantes}
#    â€¢ Statut : {"âœ… SUCCÃˆS" if urls_restantes == 0 else "âš ï¸ Ã‰CHEC"}

# ğŸ“ EXEMPLES D'URLS SUPPRIMÃ‰ES:
# """

# # Ajouter des exemples au rapport
# for i, ex in enumerate(exemples_urls[:5]):
#     rapport += f"\n   {i+1}. URLs: {', '.join(ex['urls'])}"
#     rapport += f"\n      Texte: {ex['texte'][:100]}...\n"

# # Sauvegarder le rapport
# os.makedirs("donnees/resultats", exist_ok=True)
# rapport_path = "donnees/resultats/rapport_urls.txt"
# with open(rapport_path, "w", encoding="utf-8") as f:
#     f.write(rapport)

# print(f"âœ… Rapport sauvegardÃ©: {rapport_path}")

# # 9. EXPORT OPTIONNEL
# print("\nğŸ“ EXPORT OPTIONNEL EN CSV/EXCEL...")

# reponse = input("\nVoulez-vous aussi exporter en CSV/Excel ? (o/n): ")
# if reponse.lower() == 'o':
#     try:
#         # RÃ©cupÃ©rer quelques documents
#         docs_export = list(collection_dest.find().limit(1000))
        
#         # Convertir ObjectId en string
#         for doc in docs_export:
#             doc['_id'] = str(doc['_id'])
#             if '_nettoyage' in doc:
#                 doc['_nettoyage'] = str(doc['_nettoyage'])
        
#         # CrÃ©er DataFrame
#         df_export = pd.DataFrame(docs_export)
        
#         # Exporter
#         csv_path = "donnees/resultats/commentaires_sans_urls.csv"
#         excel_path = "donnees/resultats/commentaires_sans_urls.xlsx"
        
#         df_export.to_csv(csv_path, index=False, encoding='utf-8-sig')
#         df_export.to_excel(excel_path, index=False)
        
#         print(f"âœ… CSV exportÃ©: {csv_path}")
#         print(f"âœ… Excel exportÃ©: {excel_path}")
        
#     except Exception as e:
#         print(f"âŒ Erreur lors de l'export: {e}")

# # 10. RÃ‰SUMÃ‰ FINAL
# print("\n" + "="*70)
# print("ğŸ“Š RÃ‰SUMÃ‰ FINAL - VERSION AMÃ‰LIORÃ‰E")
# print("="*70)
# print(f"ğŸ“¥ Commentaires traitÃ©s    : {len(data)}")
# print(f"ğŸ”— URLs dÃ©tectÃ©es          : {total_urls_trouvees}")
# print(f"ğŸ“ Commentaires avec URLs  : {total_avec_urls}")
# print(f"âœ… Documents modifiÃ©s      : {docs_avec_modifications}")
# print(f"ğŸ” URLs restantes          : {urls_restantes}")
# print(f"\nğŸ“ Base de donnÃ©es MongoDB:")
# print(f"   â€¢ Source : telecom_algerie.commentaires_bruts")
# print(f"   â€¢ Destination : telecom_algerie.commentaires_sans_urls")
# print("="*70)

# print("\nğŸ‰ Ã‰TAPE 1 TERMINÃ‰E AVEC SUCCÃˆS !")
# print(f"ğŸ’¡ Les commentaires nettoyÃ©s sont dans: telecom_algerie.commentaires_sans_urls")

# # Fermer les connexions
# spark.stop()
# client.close()
# print("\nğŸ”Œ Connexions fermÃ©es")

#!/usr/bin/env python3
# -*- coding: utf-8 -*-

# scripts/nettoyage/01_supprimer_urls.py - VERSION AVEC MESURE DE TEMPS

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, udf
from pyspark.sql.types import StringType, IntegerType
import pandas as pd
import re
import os
from pymongo import MongoClient
from datetime import datetime
from bson import ObjectId
import time  # ğŸ‘ˆ IMPORT POUR MESURER LE TEMPS

def supprimer_urls(texte):
    """Supprime les URLs d'un texte - Version amÃ©liorÃ©e"""
    if texte is None or not isinstance(texte, str):
        return texte
    
    # PATTERNS AMÃ‰LIORÃ‰S pour dÃ©tecter tous les types d'URLs
    patterns = [
        r'https?://\S+',           # URLs complÃ¨tes (http://example.com)
        r'www\.\S+',                # www.example.com
        r'https?://(?:\s|$)',       # https:// seul suivi d'espace ou fin de ligne
        r'https?://$',              # https:// en fin de chaÃ®ne
        r'\bhttps?://\b',           # https:// comme mot isolÃ©
        r'http://(?:\s|$)',         # http:// seul
        r'http://$'                 # http:// en fin de chaÃ®ne
    ]
    
    texte_propre = texte
    for pattern in patterns:
        texte_propre = re.sub(pattern, '', texte_propre, flags=re.IGNORECASE)
    
    # Supprimer les espaces multiples
    texte_propre = re.sub(r'\s+', ' ', texte_propre).strip()
    return texte_propre if texte_propre else None

def detecter_urls(texte):
    """DÃ©tecte si un texte contient des URLs - Version amÃ©liorÃ©e"""
    if texte is None or not isinstance(texte, str):
        return 0
    
    # Patterns de dÃ©tection
    patterns = [
        r'https?://',
        r'www\.',
        r'https?://(?:\s|$)',
        r'https?://$'
    ]
    
    for pattern in patterns:
        if re.search(pattern, texte, re.IGNORECASE):
            return 1
    return 0

def extraire_urls(texte):
    """Extrait toutes les URLs d'un texte - Version amÃ©liorÃ©e"""
    if texte is None or not isinstance(texte, str):
        return []
    
    patterns = [
        r'https?://\S+',
        r'www\.\S+',
        r'https?://(?:\s|$)',
        r'https?://$'
    ]
    
    urls = []
    for pattern in patterns:
        found = re.findall(pattern, texte, re.IGNORECASE)
        urls.extend([u for u in found if u.strip()])  # Ã‰viter les chaÃ®nes vides
    
    return list(set(urls))  # Ã‰liminer les doublons

def compter_urls(texte):
    """Compte le nombre d'URLs dans un texte"""
    if texte is None or not isinstance(texte, str):
        return 0
    return len(extraire_urls(texte))

# ğŸ“Š DÃ‰BUT DU CHRONOMÃˆTRAGE GLOBAL
temps_debut_global = time.time()

print("="*70)
print("ğŸ” Ã‰TAPE 1 : DÃ‰TECTION ET SUPPRESSION DES URLS - SINGLE NODE")
print("="*70)

# 1. CrÃ©er Spark
print("\nâš¡ DÃ©marrage de Spark...")
temps_debut_spark = time.time()

spark = SparkSession.builder \
    .appName("Suppression_URLs") \
    .master("local[*]") \
    .config("spark.executor.memory", "4g") \
    .config("spark.driver.memory", "4g") \
    .getOrCreate()

temps_fin_spark = time.time()
print(f"âœ… Spark dÃ©marrÃ© en {temps_fin_spark - temps_debut_spark:.2f} secondes")

# 2. Connexion Ã  MongoDB
print("\nğŸ“‚ Connexion Ã  MongoDB...")
temps_debut_mongo = time.time()

try:
    client = MongoClient('localhost', 27018)
    db = client['telecom_algerie']
    
    # Collection source
    collection_source = db['commentaires_bruts']
    
    # Collection destination (nettoyÃ©e)
    collection_dest = db['commentaires_sans_urls_singlenode']
    
    # Vider la collection de destination si elle existe
    collection_dest.delete_many({})
    
    temps_fin_mongo = time.time()
    print(f"âœ… Connexion MongoDB rÃ©ussie en {temps_fin_mongo - temps_debut_mongo:.2f} secondes")
    
except Exception as e:
    print(f"âŒ Erreur de connexion MongoDB: {e}")
    spark.stop()
    exit(1)

# 3. Charger les donnÃ©es
print("\nğŸ“¥ Chargement des commentaires...")
temps_debut_chargement = time.time()

data = list(collection_source.find({}))
print(f"ğŸ“Š {len(data)} commentaires chargÃ©s")

temps_fin_chargement = time.time()
print(f"âœ… Chargement terminÃ© en {temps_fin_chargement - temps_debut_chargement:.2f} secondes")

if len(data) == 0:
    print("âŒ Aucune donnÃ©e trouvÃ©e")
    spark.stop()
    exit(1)

# 4. ANALYSE : DÃ©tecter les URLs
print("\nğŸ” ANALYSE : Recherche des URLs (version amÃ©liorÃ©e)...")
temps_debut_analyse = time.time()

total_avec_urls = 0
total_urls_trouvees = 0
exemples_urls = []
cas_speciaux = []  # Pour capturer les cas comme "https://" seul

for doc in data[:50]:  # Analyser plus d'exemples pour trouver les cas spÃ©ciaux
    commentaire = doc.get('Commentaire_Client', '')
    urls = extraire_urls(commentaire)
    
    # VÃ©rifier spÃ©cifiquement les cas "https://" seul
    if re.search(r'https?://(?:\s|$)', commentaire, re.IGNORECASE):
        cas_speciaux.append({
            'texte': commentaire[:150],
            'urls': urls
        })
    
    if urls:
        exemples_urls.append({
            'texte': commentaire[:150],
            'urls': urls
        })

# Compter tous les URLs
for doc in data:
    commentaire = doc.get('Commentaire_Client', '')
    urls = extraire_urls(commentaire)
    if urls:
        total_avec_urls += 1
        total_urls_trouvees += len(urls)

temps_fin_analyse = time.time()
print(f"\nğŸ“Š STATISTIQUES (analyse en {temps_fin_analyse - temps_debut_analyse:.2f}s):")
print(f"   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
print(f"   â”‚ Total commentaires    : {len(data):<15} â”‚")
print(f"   â”‚ Avec URLs             : {total_avec_urls:<15} â”‚")
print(f"   â”‚ URLs trouvÃ©es         : {total_urls_trouvees:<15} â”‚")
print(f"   â”‚ Pourcentage           : {(total_avec_urls/len(data)*100):<15.2f}% â”‚")
print(f"   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")

# Afficher les cas spÃ©ciaux (https:// seul)
if cas_speciaux:
    print(f"\nâš ï¸ CAS SPÃ‰CIAUX DÃ‰TECTÃ‰S (https:// seul):")
    for i, ex in enumerate(cas_speciaux[:3], 1):
        print(f"\n   Cas {i}:")
        print(f"   ğŸ“ Texte: {ex['texte']}")
        print(f"   ğŸ”— URLs: {', '.join(ex['urls'])}")
        print("   " + "-" * 60)

# Afficher des exemples gÃ©nÃ©raux
if exemples_urls:
    print("\nğŸ“ EXEMPLES DE COMMENTAIRES AVEC URLS:")
    for i, ex in enumerate(exemples_urls[:5], 1):
        print(f"\n   Exemple {i}:")
        print(f"   ğŸ“ Texte: {ex['texte']}...")
        print(f"   ğŸ”— URLs: {', '.join(ex['urls'])}")
        print("   " + "-" * 60)

# 5. NETTOYAGE : Supprimer les URLs
print("\nğŸ§¹ SUPPRESSION DES URLS EN COURS...")
temps_debut_nettoyage = time.time()

docs_nettoyes = []
docs_avec_modifications = 0
urls_par_doc = []

for i, doc in enumerate(data):
    # CrÃ©er une copie du document
    doc_propre = doc.copy()
    
    # Nettoyer le commentaire client
    commentaire_original = doc.get('Commentaire_Client', '')
    commentaire_nettoye = supprimer_urls(commentaire_original)
    
    # Compter les URLs avant/aprÃ¨s
    urls_avant = compter_urls(commentaire_original)
    urls_apres = compter_urls(commentaire_nettoye)
    
    if urls_avant > 0:
        urls_par_doc.append({
            'id': doc.get('_id'),
            'avant': urls_avant,
            'apres': urls_apres,
            'texte': commentaire_original[:100]
        })
    
    if commentaire_original != commentaire_nettoye:
        docs_avec_modifications += 1
    
    doc_propre['Commentaire_Client'] = commentaire_nettoye
    
    # Nettoyer le commentaire moderateur s'il existe
    if 'commentaire_moderateur' in doc:
        mod_original = doc.get('commentaire_moderateur', '')
        mod_nettoye = supprimer_urls(mod_original)
        doc_propre['commentaire_moderateur'] = mod_nettoye
    
    # Ajouter des mÃ©tadonnÃ©es de nettoyage dÃ©taillÃ©es
    doc_propre['_nettoyage'] = {
        'date_nettoyage': datetime.now(),
        'etape': 'suppression_urls_v2',
        'urls_avant': urls_avant,
        'urls_apres': urls_apres,
        'urls_supprimees': urls_avant > 0
    }
    
    docs_nettoyes.append(doc_propre)
    
    # Afficher la progression
    if (i + 1) % 5000 == 0:
        print(f"   âœ“ {i + 1}/{len(data)} documents traitÃ©s")

temps_fin_nettoyage = time.time()
print(f"\nâœ… Traitement terminÃ©: {len(docs_nettoyes)} documents en {temps_fin_nettoyage - temps_debut_nettoyage:.2f} secondes")
print(f"   â€¢ Documents modifiÃ©s: {docs_avec_modifications}")

# Afficher quelques statistiques sur les URLs par document
if urls_par_doc:
    print("\nğŸ“Š DÃ©tail des URLs par document (Ã©chantillon):")
    for item in urls_par_doc[:5]:
        print(f"   â€¢ Document {item['id']}: {item['avant']} URLs â†’ {item['apres']} aprÃ¨s")
        print(f"     Texte: {item['texte']}...")

# 6. SAUVEGARDE DANS MONGODB
print("\nğŸ’¾ SAUVEGARDE DANS MongoDB...")
temps_debut_sauvegarde = time.time()

try:
    # InsÃ©rer par lots de 1000
    batch_size = 1000
    for i in range(0, len(docs_nettoyes), batch_size):
        batch = docs_nettoyes[i:i+batch_size]
        collection_dest.insert_many(batch)
        print(f"   âœ“ Lot {i//batch_size + 1}: {len(batch)} documents sauvegardÃ©s")
    
    temps_fin_sauvegarde = time.time()
    print(f"\nâœ… {len(docs_nettoyes)} documents sauvegardÃ©s dans 'commentaires_sans_urls_singlenode'")
    print(f"   â±ï¸  Temps de sauvegarde: {temps_fin_sauvegarde - temps_debut_sauvegarde:.2f} secondes")
    
except Exception as e:
    print(f"âŒ Erreur lors de la sauvegarde: {e}")

# 7. VÃ‰RIFICATION APPROFONDIE
print("\nğŸ” VÃ‰RIFICATION APPROFONDIE DE LA SUPPRESSION...")
temps_debut_verification = time.time()

# VÃ©rifier avec diffÃ©rents patterns
patterns_verification = [
    r'https?://\S+',
    r'www\.\S+',
    r'https?://(?:\s|$)',
    r'https?://$'
]

print("\nğŸ“Š VÃ©rification pattern par pattern:")

for pattern in patterns_verification:
    count = collection_dest.count_documents({
        "Commentaire_Client": {"$regex": pattern, "$options": "i"}
    })
    print(f"   â€¢ Pattern '{pattern[:20]}...': {count} documents")

# VÃ©rification globale
urls_restantes = collection_dest.count_documents({
    "Commentaire_Client": {"$regex": "https?://|www\.", "$options": "i"}
})

temps_fin_verification = time.time()
print(f"\nğŸ“Š RÃ‰SULTAT DE LA VÃ‰RIFICATION (en {temps_fin_verification - temps_debut_verification:.2f}s):")
print(f"   â€¢ Total documents avec URLs restantes: {urls_restantes}")

if urls_restantes == 0:
    print("   âœ… SUCCÃˆS : Aucune URL restante dÃ©tectÃ©e !")
else:
    print(f"   âš ï¸ ATTENTION : {urls_restantes} documents ont encore des URLs")
    
    # Afficher les documents problÃ©matiques
    print("\nğŸ“ DOCUMENTS AVEC URLS RESTANTES:")
    docs_problemes = collection_dest.find({
        "Commentaire_Client": {"$regex": "https?://|www\.", "$options": "i"}
    }).limit(5)
    
    for doc in docs_problemes:
        print(f"\n   â€¢ ID: {doc['_id']}")
        print(f"     Texte: {doc.get('Commentaire_Client', '')[:150]}...")

# ğŸ FIN DU CHRONOMÃˆTRAGE GLOBAL
temps_fin_global = time.time()
temps_total = temps_fin_global - temps_debut_global

# 8. CRÃ‰ER UN RAPPORT DÃ‰TAILLÃ‰ AVEC TEMPS
print("\nğŸ“„ CRÃ‰ATION DU RAPPORT...")

rapport = f"""
{"="*70}
RAPPORT DE SUPPRESSION DES URLS - SINGLE NODE
{"="*70}

Date d'exÃ©cution : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
Mode : Single Node (Spark local)

â±ï¸  TEMPS D'EXÃ‰CUTION:
   â€¢ Connexion Spark        : {temps_fin_spark - temps_debut_spark:.2f}s
   â€¢ Connexion MongoDB      : {temps_fin_mongo - temps_debut_mongo:.2f}s
   â€¢ Chargement donnÃ©es     : {temps_fin_chargement - temps_debut_chargement:.2f}s
   â€¢ Analyse des URLs       : {temps_fin_analyse - temps_debut_analyse:.2f}s
   â€¢ Nettoyage URLs         : {temps_fin_nettoyage - temps_debut_nettoyage:.2f}s
   â€¢ Sauvegarde MongoDB     : {temps_fin_sauvegarde - temps_debut_sauvegarde:.2f}s
   â€¢ VÃ©rification           : {temps_fin_verification - temps_debut_verification:.2f}s
   â€¢ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   â€¢ TEMPS TOTAL            : {temps_total:.2f}s
   â€¢ Documents par seconde  : {len(data) / temps_total:.2f} doc/s

ğŸ“Š STATISTIQUES GLOBALES:
   â€¢ Total commentaires traitÃ©s : {len(data)}
   â€¢ Commentaires avec URLs     : {total_avec_urls}
   â€¢ URLs trouvÃ©es              : {total_urls_trouvees}
   â€¢ Pourcentage avec URLs      : {total_avec_urls/len(data)*100:.2f}%
   â€¢ Documents modifiÃ©s         : {docs_avec_modifications}

ğŸ“ STOCKAGE:
   â€¢ Collection source      : telecom_algerie.commentaires_bruts
   â€¢ Collection destination : telecom_algerie.commentaires_sans_urls_singlenode
   â€¢ Documents sauvegardÃ©s  : {len(docs_nettoyes)}

ğŸ” RÃ‰SULTATS DE LA VÃ‰RIFICATION:
   â€¢ Documents avec URLs restantes : {urls_restantes}
   â€¢ Statut : {"âœ… SUCCÃˆS" if urls_restantes == 0 else "âš ï¸ Ã‰CHEC"}

ğŸ“ EXEMPLES D'URLS SUPPRIMÃ‰ES:
"""

# Ajouter des exemples au rapport
for i, ex in enumerate(exemples_urls[:5]):
    rapport += f"\n   {i+1}. URLs: {', '.join(ex['urls'])}"
    rapport += f"\n      Texte: {ex['texte'][:100]}...\n"

# Sauvegarder le rapport
os.makedirs("donnees/resultats", exist_ok=True)
rapport_path = "donnees/resultats/rapport_urls_singlenode.txt"
with open(rapport_path, "w", encoding="utf-8") as f:
    f.write(rapport)

print(f"âœ… Rapport sauvegardÃ©: {rapport_path}")

# 9. EXPORT OPTIONNEL
print("\nğŸ“ EXPORT OPTIONNEL EN CSV/EXCEL...")

reponse = input("\nVoulez-vous aussi exporter en CSV/Excel ? (o/n): ")
if reponse.lower() == 'o':
    try:
        # RÃ©cupÃ©rer quelques documents
        docs_export = list(collection_dest.find().limit(1000))
        
        # Convertir ObjectId en string
        for doc in docs_export:
            doc['_id'] = str(doc['_id'])
            if '_nettoyage' in doc:
                doc['_nettoyage'] = str(doc['_nettoyage'])
        
        # CrÃ©er DataFrame
        df_export = pd.DataFrame(docs_export)
        
        # Exporter
        csv_path = "donnees/resultats/commentaires_sans_urls_singlenode.csv"
        excel_path = "donnees/resultats/commentaires_sans_urls_singlenode.xlsx"
        
        df_export.to_csv(csv_path, index=False, encoding='utf-8-sig')
        df_export.to_excel(excel_path, index=False)
        
        print(f"âœ… CSV exportÃ©: {csv_path}")
        print(f"âœ… Excel exportÃ©: {excel_path}")
        
    except Exception as e:
        print(f"âŒ Erreur lors de l'export: {e}")

# 10. RÃ‰SUMÃ‰ FINAL AVEC TEMPS
print("\n" + "="*70)
print("ğŸ“Š RÃ‰SUMÃ‰ FINAL - SINGLE NODE")
print("="*70)
print(f"ğŸ“¥ Commentaires traitÃ©s    : {len(data)}")
print(f"ğŸ”— URLs dÃ©tectÃ©es          : {total_urls_trouvees}")
print(f"ğŸ“ Commentaires avec URLs  : {total_avec_urls}")
print(f"âœ… Documents modifiÃ©s      : {docs_avec_modifications}")
print(f"ğŸ” URLs restantes          : {urls_restantes}")
print(f"\nâ±ï¸  TEMPS D'EXÃ‰CUTION:")
print(f"   â€¢ Chargement : {temps_fin_chargement - temps_debut_chargement:.2f}s")
print(f"   â€¢ Nettoyage  : {temps_fin_nettoyage - temps_debut_nettoyage:.2f}s")
print(f"   â€¢ Sauvegarde : {temps_fin_sauvegarde - temps_debut_sauvegarde:.2f}s")
print(f"   â€¢ TOTAL      : {temps_total:.2f}s")
print(f"   â€¢ Vitesse    : {len(data) / temps_total:.2f} docs/s")
print(f"\nğŸ“ Base de donnÃ©es MongoDB:")
print(f"   â€¢ Source : telecom_algerie.commentaires_bruts")
print(f"   â€¢ Destination : telecom_algerie.commentaires_sans_urls_singlenode")
print("="*70)

print("\nğŸ‰ Ã‰TAPE 1 TERMINÃ‰E AVEC SUCCÃˆS !")
print(f"ğŸ’¡ Les commentaires nettoyÃ©s sont dans: telecom_algerie.commentaires_sans_urls_singlenode")

# Fermer les connexions
spark.stop()
client.close()
print("\nğŸ”Œ Connexions fermÃ©es")