# services:
#   mongodb:
#     image: mongo:latest
#     container_name: mongodb_pfe
#     ports:
#       - "27018:27017"
#     restart: always

#   spark-master:
#     image: apache/spark:latest
#     container_name: spark_master_pfe
#     environment:
#       - SPARK_MODE=master
#     ports:
#       - "8080:8080"
#       - "7077:7077"
#     # Commande spécifique pour garder l'image Apache vivante
#     command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
#     restart: always

#   spark-worker:
#     image: apache/spark:latest
#     container_name: worker_1_pfe
#     environment:
#       - SPARK_MODE=worker
#       - SPARK_MASTER_URL=spark://spark-master:7077
#       - spark-master
#     command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
#     restart: always

# # --- LE DEUXIÈME WORKER AJOUTÉ ICI ---
#   spark-worker-2:
#     image: apache/spark:latest
#     container_name: worker_2_pfe
#     environment:
#       - SPARK_MODE=worker
#       - SPARK_MASTER_URL=spark://spark-master:7077
#     depends_on:
#       - spark-master
#     command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
#     restart: always
services:
  mongodb:
    image: mongo:latest
    container_name: mongodb_pfe
    ports:
      - "27018:27017"
    volumes:
      - mongodb-data:/data/db  # Pour persister les données MongoDB
    networks:
      - spark_network
    restart: always

  spark-master:
    image: apache/spark:latest
    container_name: spark_master_pfe
    environment:
      - SPARK_MODE=master
    ports:
      - "8080:8080"
      - "7077:7077"
    volumes:
      - spark-data:/data
      - ./data/results:/opt/data  # Volume pour les résultats
    networks:
      - spark_network
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
    restart: always

  spark-worker-1:
    image: apache/spark:latest
    container_name: worker_1_pfe
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
    volumes:
      - spark-data:/data
      - ./data/results:/opt/data  # MÊME volume pour les résultats
    networks:
      - spark_network
    depends_on:
      - spark-master
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    restart: always

  spark-worker-2:
    image: apache/spark:latest
    container_name: worker_2_pfe
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
    volumes:
      - spark-data:/data
      - ./data/results:/opt/data  # MÊME volume pour les résultats
    networks:
      - spark_network
    depends_on:
      - spark-master
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    restart: always

volumes:
  spark-data:
  mongodb-data:  # Volume pour MongoDB

networks:
  spark_network:
    driver: bridge
